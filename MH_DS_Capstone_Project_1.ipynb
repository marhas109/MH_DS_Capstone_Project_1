{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Add a relevant banner image here](path_to_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Short project description. Your bottom line up front (BLUF) insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "Text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load relevant imports here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.15 GiB for an array with shape (20, 7728394) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_all_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mData/US_Accidents_March23.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_all_data\u001b[38;5;241m.\u001b[39mdescribe())\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_all_data\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1721\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1718\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1719\u001b[0m         new_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index)\n\u001b[1;32m-> 1721\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1723\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_rows\n\u001b[0;32m   1724\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\core\\frame.py:709\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    703\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    704\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    705\u001b[0m     )\n\u001b[0;32m    707\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    708\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 709\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\core\\internals\\construction.py:481\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    478\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    479\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\core\\internals\\construction.py:153\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    150\u001b[0m axes \u001b[38;5;241m=\u001b[39m [columns, index]\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_block_manager_from_column_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefs\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2142\u001b[0m, in \u001b[0;36mcreate_block_manager_from_column_arrays\u001b[1;34m(arrays, axes, consolidate, refs)\u001b[0m\n\u001b[0;32m   2140\u001b[0m     raise_construction_error(\u001b[38;5;28mlen\u001b[39m(arrays), arrays[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, axes, e)\n\u001b[0;32m   2141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m consolidate:\n\u001b[1;32m-> 2142\u001b[0m     \u001b[43mmgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mgr\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1829\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1823\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1824\u001b[0m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[0;32m   1825\u001b[0m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[0;32m   1826\u001b[0m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[0;32m   1827\u001b[0m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[1;32m-> 1829\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1830\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1831\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2272\u001b[0m, in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2270\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2271\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[1;32m-> 2272\u001b[0m     merged_blocks, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2273\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[0;32m   2274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2275\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[0;32m   2276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2304\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2301\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m bvals2[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_concat_same_type(bvals2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2303\u001b[0m argsort \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(new_mgr_locs)\n\u001b[1;32m-> 2304\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mnew_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43margsort\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2305\u001b[0m new_mgr_locs \u001b[38;5;241m=\u001b[39m new_mgr_locs[argsort]\n\u001b[0;32m   2307\u001b[0m bp \u001b[38;5;241m=\u001b[39m BlockPlacement(new_mgr_locs)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.15 GiB for an array with shape (20, 7728394) and data type object"
     ]
    }
   ],
   "source": [
    "df_all_data = pd.read_csv('Data/US_Accidents_March23.csv')\n",
    "print(df_all_data.describe())\n",
    "print(df_all_data.head())\n",
    "print(df_all_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_severity_by_state = pd.crosstab(df_all_data['Severity'], df_all_data['State'])\n",
    "df_severity_by_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart of accident severity by state\n",
    "states = df_severity_by_state.columns\n",
    "sev1 = df_severity_by_state.iloc[0]\n",
    "sev2 = df_severity_by_state.iloc[1]\n",
    "sev3 = df_severity_by_state.iloc[2]\n",
    "sev4 = ct_sev4_by_state = df_severity_by_state.iloc[3]\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.bar(states, sev1)\n",
    "plt.bar(states, sev2, bottom=sev1)\n",
    "plt.bar(states, sev3, bottom=sev1+sev2)\n",
    "plt.bar(states, sev4, bottom=sev1+sev2+sev3)\n",
    "plt.xlabel(\"State\")\n",
    "plt.xticks(rotation=60)\n",
    "plt.ylabel(\"Number of Accidents\")\n",
    "plt.legend([\"Severity 1\", \"Severity 2\", \"Severity 3\", \"Severity 4\"])\n",
    "plt.title(\"Accident Severity by State (2016 - 2023)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart of accident severity by state without sev2 (sev 2 >> than the others so obscures sev 1, 3, 4 above)\n",
    "states = df_severity_by_state.columns\n",
    "sev1 = df_severity_by_state.iloc[0]\n",
    "sev3 = df_severity_by_state.iloc[2]\n",
    "sev4 = ct_sev4_by_state = df_severity_by_state.iloc[3]\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.bar(states, sev1)\n",
    "plt.bar(states, sev3)\n",
    "plt.bar(states, sev4)\n",
    "plt.xlabel(\"State\")\n",
    "plt.xticks(rotation=60)\n",
    "plt.ylabel(\"Number of Accidents\")\n",
    "plt.legend([\"Severity 1\", \"Severity 3\", \"Severity 4\"])\n",
    "plt.title(\"Accident Severity by State, Excluding Severity 2 (2016 - 2023)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sev_by_crossing = pd.crosstab(df_all_data['Severity'], df_all_data['Crossing'])\n",
    "df_sev_by_crossing = df_sev_by_crossing.rename(columns={False: \"No\", True: \"Yes\"})\n",
    "df_sev_by_crossing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# charts of accident severity by bump, traffic calming, roundabout\n",
    "# chart of accident severity by state\n",
    "crossing = df_sev_by_crossing.columns\n",
    "cr1 = df_sev_by_crossing.iloc[0]\n",
    "cr2 = df_sev_by_crossing.iloc[1]\n",
    "cr3 = df_sev_by_crossing.iloc[2]\n",
    "cr4 = df_sev_by_crossing.iloc[3]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.bar(crossing, cr1)\n",
    "plt.bar(crossing, cr2)\n",
    "plt.bar(crossing, cr3)\n",
    "plt.bar(crossing, cr4)\n",
    "plt.xticks(rotation=60)\n",
    "plt.xlabel(\"Nearby Crossing\")\n",
    "plt.ylabel(\"Number of Accidents\")\n",
    "plt.legend([\"Severity 1\", \"Severity 2\", \"Severity 3\", \"Severity 4\"])\n",
    "plt.title(\"Accident Severity by Proximity to Crossing, (2016 - 2023)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Count of Wind Direction Entries: {df_all_data['Wind_Direction'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Selection\n",
    "\n",
    "Based on my exploration of the data, I'm dropping the following fields from the dataset for the following reasons:\n",
    "\n",
    "- Source: contains information that has no relationship to causes and effects of accidents\n",
    "- Timezone: duplicates zip/state with less precision\n",
    "- Country: all data is from the United States so this field is redundant\n",
    "- Airport_Code: doesn't provide germane information-the exact location where weather conditions are reported is not a variable that can be adjusted\n",
    "- Weather_Timestamp: not related to the conditions of the accidents in any way\n",
    "- Wind_Direction: too many unique values; values are also not related to travel directions so it's unlikely they'll  produce clear/actionable conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Severity     Start_Lat     Start_Lng       End_Lat       End_Lng  \\\n",
      "count  7.728394e+06  7.728394e+06  7.728394e+06  4.325632e+06  4.325632e+06   \n",
      "mean   2.212384e+00  3.620119e+01 -9.470255e+01  3.626183e+01 -9.572557e+01   \n",
      "std    4.875313e-01  5.076079e+00  1.739176e+01  5.272905e+00  1.810793e+01   \n",
      "min    1.000000e+00  2.455480e+01 -1.246238e+02  2.456601e+01 -1.245457e+02   \n",
      "25%    2.000000e+00  3.339963e+01 -1.172194e+02  3.346207e+01 -1.177543e+02   \n",
      "50%    2.000000e+00  3.582397e+01 -8.776662e+01  3.618349e+01 -8.802789e+01   \n",
      "75%    2.000000e+00  4.008496e+01 -8.035368e+01  4.017892e+01 -8.024709e+01   \n",
      "max    4.000000e+00  4.900220e+01 -6.711317e+01  4.907500e+01 -6.710924e+01   \n",
      "\n",
      "       Distance(mi)  Temperature(F)  Wind_Chill(F)   Humidity(%)  \\\n",
      "count  7.728394e+06    7.564541e+06   5.729375e+06  7.554250e+06   \n",
      "mean   5.618423e-01    6.166329e+01   5.825105e+01  6.483104e+01   \n",
      "std    1.776811e+00    1.901365e+01   2.238983e+01  2.282097e+01   \n",
      "min    0.000000e+00   -8.900000e+01  -8.900000e+01  1.000000e+00   \n",
      "25%    0.000000e+00    4.900000e+01   4.300000e+01  4.800000e+01   \n",
      "50%    3.000000e-02    6.400000e+01   6.200000e+01  6.700000e+01   \n",
      "75%    4.640000e-01    7.600000e+01   7.500000e+01  8.400000e+01   \n",
      "max    4.417500e+02    2.070000e+02   2.070000e+02  1.000000e+02   \n",
      "\n",
      "       Pressure(in)  Visibility(mi)  Wind_Speed(mph)  Precipitation(in)  \n",
      "count  7.587715e+06    7.551296e+06     7.157161e+06       5.524808e+06  \n",
      "mean   2.953899e+01    9.090376e+00     7.685490e+00       8.407210e-03  \n",
      "std    1.006190e+00    2.688316e+00     5.424983e+00       1.102246e-01  \n",
      "min    0.000000e+00    0.000000e+00     0.000000e+00       0.000000e+00  \n",
      "25%    2.937000e+01    1.000000e+01     4.600000e+00       0.000000e+00  \n",
      "50%    2.986000e+01    1.000000e+01     7.000000e+00       0.000000e+00  \n",
      "75%    3.003000e+01    1.000000e+01     1.040000e+01       0.000000e+00  \n",
      "max    5.863000e+01    1.400000e+02     1.087000e+03       3.647000e+01  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7728394 entries, 0 to 7728393\n",
      "Data columns (total 40 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   ID                     object \n",
      " 1   Severity               int64  \n",
      " 2   Start_Time             object \n",
      " 3   End_Time               object \n",
      " 4   Start_Lat              float64\n",
      " 5   Start_Lng              float64\n",
      " 6   End_Lat                float64\n",
      " 7   End_Lng                float64\n",
      " 8   Distance(mi)           float64\n",
      " 9   Description            object \n",
      " 10  Street                 object \n",
      " 11  City                   object \n",
      " 12  County                 object \n",
      " 13  State                  object \n",
      " 14  Zipcode                object \n",
      " 15  Temperature(F)         float64\n",
      " 16  Wind_Chill(F)          float64\n",
      " 17  Humidity(%)            float64\n",
      " 18  Pressure(in)           float64\n",
      " 19  Visibility(mi)         float64\n",
      " 20  Wind_Speed(mph)        float64\n",
      " 21  Precipitation(in)      float64\n",
      " 22  Weather_Condition      object \n",
      " 23  Amenity                bool   \n",
      " 24  Bump                   bool   \n",
      " 25  Crossing               bool   \n",
      " 26  Give_Way               bool   \n",
      " 27  Junction               bool   \n",
      " 28  No_Exit                bool   \n",
      " 29  Railway                bool   \n",
      " 30  Roundabout             bool   \n",
      " 31  Station                bool   \n",
      " 32  Stop                   bool   \n",
      " 33  Traffic_Calming        bool   \n",
      " 34  Traffic_Signal         bool   \n",
      " 35  Turning_Loop           bool   \n",
      " 36  Sunrise_Sunset         object \n",
      " 37  Civil_Twilight         object \n",
      " 38  Nautical_Twilight      object \n",
      " 39  Astronomical_Twilight  object \n",
      "dtypes: bool(13), float64(12), int64(1), object(14)\n",
      "memory usage: 1.6+ GB\n",
      "None\n",
      "Index(['ID', 'Severity', 'Start_Time', 'End_Time', 'Start_Lat', 'Start_Lng',\n",
      "       'End_Lat', 'End_Lng', 'Distance(mi)', 'Description', 'Street', 'City',\n",
      "       'County', 'State', 'Zipcode', 'Temperature(F)', 'Wind_Chill(F)',\n",
      "       'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)',\n",
      "       'Precipitation(in)', 'Weather_Condition', 'Amenity', 'Bump', 'Crossing',\n",
      "       'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station',\n",
      "       'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop',\n",
      "       'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight',\n",
      "       'Astronomical_Twilight'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_refined = df_all_data.drop(['Source', 'Timezone', 'Country', 'Airport_Code', 'Weather_Timestamp', 'Wind_Direction'], axis=1)\n",
    "\n",
    "print(df_refined.describe())\n",
    "print(df_refined.info())\n",
    "print(df_refined.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refined['Start_Time'] = pd.to_datetime(df_refined['Start_Time'], yearfirst=True, format='mixed')\n",
    "df_refined['End_Time'] = pd.to_datetime(df_refined['End_Time'], yearfirst=True, format='mixed')\n",
    "df_refined['Acc_date'] = df_refined['Start_Time'].dt.date\n",
    "df_refined.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: 158386 temperatures still missing after date-zip fill.\n",
      "Filling remaining with date-state mean.\n",
      "Warning: 7341 temperatures still missing after date-state fill.\n",
      "Filling remaining with overall mean as last resort.\n"
     ]
    }
   ],
   "source": [
    "# replacing NaN values in the temperature column\n",
    "\n",
    "def fill_missing_temperatures(df, date_col='Acc_date', zip_col='Zipcode', state_col='State', temp_col='Temperature(F)'):\n",
    "    \"\"\"\n",
    "    Replaces NaN Temperature values with the mean temperature of entries\n",
    "    with the same date and zip code. If no match exists with date and county,\n",
    "    uses date and state.\n",
    "    \"\"\"\n",
    "    # Step 1: mean temperature for each date-zip combination\n",
    "    df_temp_fill = df_refined.copy()\n",
    "    \n",
    "    temp_means_zip = df_temp_fill.groupby([date_col, zip_col])[temp_col].transform('mean')\n",
    "    \n",
    "    # fill NaN values with the date-zip group mean\n",
    "    df_temp_fill[temp_col] = df_temp_fill[temp_col].fillna(temp_means_zip)\n",
    "    \n",
    "    # Step 2: date-state combination for remaining NaNs\n",
    "    if df_temp_fill[temp_col].isna().any():\n",
    "        remaining = df_temp_fill[temp_col].isna().sum()\n",
    "        print(f\"Info: {remaining} temperatures still missing after date-zip fill.\")\n",
    "        print(\"Filling remaining with date-state mean.\")\n",
    "        \n",
    "        temp_means_state = df_temp_fill.groupby([date_col, state_col])[temp_col].transform('mean')\n",
    "        df_temp_fill[temp_col] = df_temp_fill[temp_col].fillna(temp_means_state)\n",
    "    \n",
    "    # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "    if df_temp_fill[temp_col].isna().any():\n",
    "        remaining = df_temp_fill[temp_col].isna().sum()\n",
    "        print(f\"Warning: {remaining} temperatures still missing after date-state fill.\")\n",
    "        print(\"Filling remaining with overall mean as last resort.\")\n",
    "        df_temp_fill[temp_col] = df_temp_fill[temp_col].fillna(df_temp_fill[temp_col].mean())\n",
    "    \n",
    "    return df_temp_fill\n",
    "\n",
    "df_temp_fill = fill_missing_temperatures(df_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying NaNs have been replaced in the working df\n",
    "df_temp_fill['Temperature(F)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# writing NaN-free column back to the original dataframe\n",
    "df_refined['Temperature(F)'] = df_temp_fill['Temperature(F)']\n",
    "df_refined['Temperature(F)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting working df to free up memory\n",
    "del df_temp_fill\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: 1949795 wind chills still missing after date-zip fill.\n",
      "Filling remaining with date-state mean.\n",
      "Warning: 687931 wind chills still missing after date-state fill.\n",
      "Filling remaining with overall mean as last resort.\n"
     ]
    }
   ],
   "source": [
    "# replacing NaN values in the wind chill column\n",
    "\n",
    "def fill_missing_windchill(df, date_col='Acc_date', zip_col='Zipcode', state_col='State', windchill_col='Wind_Chill(F)'):\n",
    "    \"\"\"\n",
    "    Replaces NaN wind chill values with the mean wind chill of entries\n",
    "    with the same date and zip code. If no match exists with date and county,\n",
    "    uses date and state.\n",
    "    \"\"\"\n",
    "    # Step 1: mean temperature for each date-zip combination\n",
    "    df_windchill_fill = df_refined.copy()\n",
    "    \n",
    "    windchill_means_zip = df_windchill_fill.groupby([date_col, zip_col])[windchill_col].transform('mean')\n",
    "    \n",
    "    # fill NaN values with the date-zip group mean\n",
    "    df_windchill_fill[windchill_col] = df_windchill_fill[windchill_col].fillna(windchill_means_zip)\n",
    "    \n",
    "    # Step 2: date-state combination for remaining NaNs\n",
    "    if df_windchill_fill[windchill_col].isna().any():\n",
    "        remaining = df_windchill_fill[windchill_col].isna().sum()\n",
    "        print(f\"Info: {remaining} wind chills still missing after date-zip fill.\")\n",
    "        print(\"Filling remaining with date-state mean.\")\n",
    "        \n",
    "        windchill_means_state = df_windchill_fill.groupby([date_col, state_col])[windchill_col].transform('mean')\n",
    "        df_windchill_fill[windchill_col] = df_windchill_fill[windchill_col].fillna(windchill_means_state)\n",
    "    \n",
    "    # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "    if df_windchill_fill[windchill_col].isna().any():\n",
    "        remaining = df_windchill_fill[windchill_col].isna().sum()\n",
    "        print(f\"Warning: {remaining} wind chills still missing after date-state fill.\")\n",
    "        print(\"Filling remaining with overall mean as last resort.\")\n",
    "        df_windchill_fill[windchill_col] = df_windchill_fill[windchill_col].fillna(df_windchill_fill[windchill_col].mean())\n",
    "    \n",
    "    return df_windchill_fill\n",
    "\n",
    "df_windchill_fill = fill_missing_windchill(df_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying NaNs have been replaced in the working df\n",
    "df_windchill_fill['Wind_Chill(F)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# writing NaN-free column back to the original dataframe\n",
    "df_refined['Wind_Chill(F)'] = df_windchill_fill['Wind_Chill(F)']\n",
    "df_refined['Wind_Chill(F)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting working df to free up memory\n",
    "del df_windchill_fill\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 708. MiB for an array with shape (12, 7728394) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 35\u001b[0m\n\u001b[0;32m     31\u001b[0m         df_hum_fill[hum_col] \u001b[38;5;241m=\u001b[39m df_hum_fill[hum_col]\u001b[38;5;241m.\u001b[39mfillna(df_hum_fill[hum_col]\u001b[38;5;241m.\u001b[39mmean())\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df_hum_fill\n\u001b[1;32m---> 35\u001b[0m df_hum_fill \u001b[38;5;241m=\u001b[39m \u001b[43mfill_missing_hum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_refined\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 10\u001b[0m, in \u001b[0;36mfill_missing_hum\u001b[1;34m(df, date_col, zip_col, state_col, hum_col)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mReplaces NaN humidity values with the mean humidity of entries\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mwith the same date and zip code. If no match exists with date and county,\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03muses date and state.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Step 1: mean humidity for each date-zip combination\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m df_hum_fill \u001b[38;5;241m=\u001b[39m \u001b[43mdf_refined\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m hum_means_zip \u001b[38;5;241m=\u001b[39m df_hum_fill\u001b[38;5;241m.\u001b[39mgroupby([date_col, zip_col])[hum_col]\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# fill NaN values with the date-zip group mean\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\core\\generic.py:6452\u001b[0m, in \u001b[0;36mNDFrame.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   6342\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   6343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m: NDFrameT, deep: bool_t \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m   6344\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6345\u001b[0m \u001b[38;5;124;03m    Make a copy of this object's indices and data.\u001b[39;00m\n\u001b[0;32m   6346\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6450\u001b[0m \u001b[38;5;124;03m    dtype: object\u001b[39;00m\n\u001b[0;32m   6451\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 6452\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n\u001b[0;32m   6454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\core\\internals\\managers.py:664\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    661\u001b[0m         res\u001b[38;5;241m.\u001b[39m_blklocs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blklocs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 664\u001b[0m     \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1829\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1823\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1824\u001b[0m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[0;32m   1825\u001b[0m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[0;32m   1826\u001b[0m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[0;32m   1827\u001b[0m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[1;32m-> 1829\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1830\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1831\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2272\u001b[0m, in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2270\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2271\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[1;32m-> 2272\u001b[0m     merged_blocks, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2273\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[0;32m   2274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2275\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[0;32m   2276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2304\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2301\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m bvals2[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_concat_same_type(bvals2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2303\u001b[0m argsort \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(new_mgr_locs)\n\u001b[1;32m-> 2304\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mnew_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43margsort\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2305\u001b[0m new_mgr_locs \u001b[38;5;241m=\u001b[39m new_mgr_locs[argsort]\n\u001b[0;32m   2307\u001b[0m bp \u001b[38;5;241m=\u001b[39m BlockPlacement(new_mgr_locs)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 708. MiB for an array with shape (12, 7728394) and data type float64"
     ]
    }
   ],
   "source": [
    "# replacing NaN values in the humidity column\n",
    "\n",
    "def fill_missing_hum(df, date_col='Acc_date', zip_col='Zipcode', state_col='State', hum_col='Humidity(%)'):\n",
    "    \"\"\"\n",
    "    Replaces NaN humidity values with the mean humidity of entries\n",
    "    with the same date and zip code. If no match exists with date and county,\n",
    "    uses date and state.\n",
    "    \"\"\"\n",
    "    # Step 1: mean humidity for each date-zip combination\n",
    "    df_hum_fill = df_refined.copy()\n",
    "    \n",
    "    hum_means_zip = df_hum_fill.groupby([date_col, zip_col])[hum_col].transform('mean')\n",
    "    \n",
    "    # fill NaN values with the date-zip group mean\n",
    "    df_hum_fill[hum_col] = df_hum_fill[hum_col].fillna(hum_means_zip)\n",
    "    \n",
    "    # Step 2: date-state combination for remaining NaNs\n",
    "    if df_hum_fill[hum_col].isna().any():\n",
    "        remaining = df_hum_fill[hum_col].isna().sum()\n",
    "        print(f\"Info: {remaining} humidity entries still missing after date-zip fill.\")\n",
    "        print(\"Filling remaining with date-state mean.\")\n",
    "        \n",
    "        hum_means_state = df_hum_fill.groupby([date_col, state_col])[hum_col].transform('mean')\n",
    "        df_hum_fill[hum_col] = df_hum_fill[hum_col].fillna(hum_means_state)\n",
    "    \n",
    "    # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "    if df_hum_fill[hum_col].isna().any():\n",
    "        remaining = df_hum_fill[hum_col].isna().sum()\n",
    "        print(f\"Warning: {remaining} humidity entries still missing after date-state fill.\")\n",
    "        print(\"Filling remaining with overall mean as last resort.\")\n",
    "        df_hum_fill[hum_col] = df_hum_fill[hum_col].fillna(df_hum_fill[hum_col].mean())\n",
    "    \n",
    "    return df_hum_fill\n",
    "\n",
    "df_hum_fill = fill_missing_hum(df_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying NaNs have been replaced in the working df\n",
    "df_hum_fill['Humidity(%)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing NaN-free column back to the original dataframe\n",
    "df_refined['Humidity(%)'] = df_hum_fill['Humidity(%)']\n",
    "df_refined['Humidity(%)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting working df to free up memory\n",
    "del df_hum_fill\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing NaN values in the pressure column\n",
    "\n",
    "def fill_missing_press(df, date_col='Acc_date', zip_col='Zipcode', state_col='State', press_col='Pressure(in)'):\n",
    "    \"\"\"\n",
    "    Replaces NaN pressure values with the mean pressure of entries\n",
    "    with the same date and zip code. If no match exists with date and county,\n",
    "    uses date and state.\n",
    "    \"\"\"\n",
    "    # Step 1: mean pressure for each date-zip combination\n",
    "    df_press_fill = df_refined.copy()\n",
    "    \n",
    "    press_means_zip = df_press_fill.groupby([date_col, zip_col])[press_col].transform('mean')\n",
    "    \n",
    "    # fill NaN values with the date-zip group mean\n",
    "    df_press_fill[press_col] = df_press_fill[press_col].fillna(press_means_zip)\n",
    "    \n",
    "    # Step 2: date-state combination for remaining NaNs\n",
    "    if df_press_fill[press_col].isna().any():\n",
    "        remaining = df_press_fill[press_col].isna().sum()\n",
    "        print(f\"Info: {remaining} pressure entries still missing after date-zip fill.\")\n",
    "        print(\"Filling remaining with date-state mean.\")\n",
    "        \n",
    "        press_means_state = df_press_fill.groupby([date_col, state_col])[press_col].transform('mean')\n",
    "        df_press_fill[press_col] = df_hum_fill[press_col].fillna(press_means_state)\n",
    "    \n",
    "    # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "    if df_press_fill[press_col].isna().any():\n",
    "        remaining = df_press_fill[press_col].isna().sum()\n",
    "        print(f\"Warning: {remaining} pressure entries still missing after date-state fill.\")\n",
    "        print(\"Filling remaining with overall mean as last resort.\")\n",
    "        df_press_fill[press_col] = df_press_fill[[press]_col].fillna(df_press_fill[press_col].mean())\n",
    "    \n",
    "    return df_hum_fill\n",
    "\n",
    "df_hum_fill = fill_missing_hum(df_refined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've managed missing values in the code blocks above. Here's a brief explanation of my approch to each column:\n",
    "\n",
    "- Temperature, Wind_Chill, Humidity, Pressure, Visibility: imputed based on the mean temp of other accident entries sharing the same day and zip code (or state if there are none in the zip code)\n",
    "- Precipitation, Wind_Speed: assumed NaN indicates no precipitation/wind and replaced NaN with zero\n",
    "- Weather_Condition: consolidated entries from 144 to \n",
    "- Sunrise_Sunset: imputed based on\n",
    "- Civil_Twilight: imputed based on\n",
    "- Nautical_Twilight: imputed based on\n",
    "- Astronomical_Twilight: imputed based on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Count of Unique Weather Condition Entries: {df_all_data['Weather_Condition'].nunique()}\")\n",
    "df_refined['Weather_Condition'].value_counts().sort_values(ascending=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Business Insight/Recommendation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Insight/Recommendation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Insight/Recommendation 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tableau Dashboard link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "Text here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
