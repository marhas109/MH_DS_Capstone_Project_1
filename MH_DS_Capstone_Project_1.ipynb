{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Add a relevant banner image here](path_to_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Short project description. Your bottom line up front (BLUF) insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "Text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load relevant imports here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Severity     Start_Lat     Start_Lng       End_Lat       End_Lng  \\\n",
      "count  7.728394e+06  7.728394e+06  7.728394e+06  4.325632e+06  4.325632e+06   \n",
      "mean   2.212384e+00  3.620119e+01 -9.470255e+01  3.626183e+01 -9.572557e+01   \n",
      "std    4.875313e-01  5.076079e+00  1.739176e+01  5.272905e+00  1.810793e+01   \n",
      "min    1.000000e+00  2.455480e+01 -1.246238e+02  2.456601e+01 -1.245457e+02   \n",
      "25%    2.000000e+00  3.339963e+01 -1.172194e+02  3.346207e+01 -1.177543e+02   \n",
      "50%    2.000000e+00  3.582397e+01 -8.776662e+01  3.618349e+01 -8.802789e+01   \n",
      "75%    2.000000e+00  4.008496e+01 -8.035368e+01  4.017892e+01 -8.024709e+01   \n",
      "max    4.000000e+00  4.900220e+01 -6.711317e+01  4.907500e+01 -6.710924e+01   \n",
      "\n",
      "       Distance(mi)  Temperature(F)  Wind_Chill(F)   Humidity(%)  \\\n",
      "count  7.728394e+06    7.564541e+06   5.729375e+06  7.554250e+06   \n",
      "mean   5.618423e-01    6.166329e+01   5.825105e+01  6.483104e+01   \n",
      "std    1.776811e+00    1.901365e+01   2.238983e+01  2.282097e+01   \n",
      "min    0.000000e+00   -8.900000e+01  -8.900000e+01  1.000000e+00   \n",
      "25%    0.000000e+00    4.900000e+01   4.300000e+01  4.800000e+01   \n",
      "50%    3.000000e-02    6.400000e+01   6.200000e+01  6.700000e+01   \n",
      "75%    4.640000e-01    7.600000e+01   7.500000e+01  8.400000e+01   \n",
      "max    4.417500e+02    2.070000e+02   2.070000e+02  1.000000e+02   \n",
      "\n",
      "       Pressure(in)  Visibility(mi)  Wind_Speed(mph)  Precipitation(in)  \n",
      "count  7.587715e+06    7.551296e+06     7.157161e+06       5.524808e+06  \n",
      "mean   2.953899e+01    9.090376e+00     7.685490e+00       8.407210e-03  \n",
      "std    1.006190e+00    2.688316e+00     5.424983e+00       1.102246e-01  \n",
      "min    0.000000e+00    0.000000e+00     0.000000e+00       0.000000e+00  \n",
      "25%    2.937000e+01    1.000000e+01     4.600000e+00       0.000000e+00  \n",
      "50%    2.986000e+01    1.000000e+01     7.000000e+00       0.000000e+00  \n",
      "75%    3.003000e+01    1.000000e+01     1.040000e+01       0.000000e+00  \n",
      "max    5.863000e+01    1.400000e+02     1.087000e+03       3.647000e+01  \n",
      "    ID   Source  Severity           Start_Time             End_Time  \\\n",
      "0  A-1  Source2         3  2016-02-08 05:46:00  2016-02-08 11:00:00   \n",
      "1  A-2  Source2         2  2016-02-08 06:07:59  2016-02-08 06:37:59   \n",
      "2  A-3  Source2         2  2016-02-08 06:49:27  2016-02-08 07:19:27   \n",
      "3  A-4  Source2         3  2016-02-08 07:23:34  2016-02-08 07:53:34   \n",
      "4  A-5  Source2         2  2016-02-08 07:39:07  2016-02-08 08:09:07   \n",
      "\n",
      "   Start_Lat  Start_Lng  End_Lat  End_Lng  Distance(mi)  ... Roundabout  \\\n",
      "0  39.865147 -84.058723      NaN      NaN          0.01  ...      False   \n",
      "1  39.928059 -82.831184      NaN      NaN          0.01  ...      False   \n",
      "2  39.063148 -84.032608      NaN      NaN          0.01  ...      False   \n",
      "3  39.747753 -84.205582      NaN      NaN          0.01  ...      False   \n",
      "4  39.627781 -84.188354      NaN      NaN          0.01  ...      False   \n",
      "\n",
      "  Station   Stop Traffic_Calming Traffic_Signal Turning_Loop Sunrise_Sunset  \\\n",
      "0   False  False           False          False        False          Night   \n",
      "1   False  False           False          False        False          Night   \n",
      "2   False  False           False           True        False          Night   \n",
      "3   False  False           False          False        False          Night   \n",
      "4   False  False           False           True        False            Day   \n",
      "\n",
      "  Civil_Twilight Nautical_Twilight Astronomical_Twilight  \n",
      "0          Night             Night                 Night  \n",
      "1          Night             Night                   Day  \n",
      "2          Night               Day                   Day  \n",
      "3            Day               Day                   Day  \n",
      "4            Day               Day                   Day  \n",
      "\n",
      "[5 rows x 46 columns]\n",
      "Index(['ID', 'Source', 'Severity', 'Start_Time', 'End_Time', 'Start_Lat',\n",
      "       'Start_Lng', 'End_Lat', 'End_Lng', 'Distance(mi)', 'Description',\n",
      "       'Street', 'City', 'County', 'State', 'Zipcode', 'Country', 'Timezone',\n",
      "       'Airport_Code', 'Weather_Timestamp', 'Temperature(F)', 'Wind_Chill(F)',\n",
      "       'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Direction',\n",
      "       'Wind_Speed(mph)', 'Precipitation(in)', 'Weather_Condition', 'Amenity',\n",
      "       'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway',\n",
      "       'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal',\n",
      "       'Turning_Loop', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight',\n",
      "       'Astronomical_Twilight'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_all_data = pd.read_csv('Data/US_Accidents_March23.csv')\n",
    "print(df_all_data.describe())\n",
    "print(df_all_data.head())\n",
    "print(df_all_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_severity_by_state = pd.crosstab(df_all_data['Severity'], df_all_data['State'])\n",
    "df_severity_by_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart of accident severity by state\n",
    "states = df_severity_by_state.columns\n",
    "sev1 = df_severity_by_state.iloc[0]\n",
    "sev2 = df_severity_by_state.iloc[1]\n",
    "sev3 = df_severity_by_state.iloc[2]\n",
    "sev4 = ct_sev4_by_state = df_severity_by_state.iloc[3]\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.bar(states, sev1)\n",
    "plt.bar(states, sev2, bottom=sev1)\n",
    "plt.bar(states, sev3, bottom=sev1+sev2)\n",
    "plt.bar(states, sev4, bottom=sev1+sev2+sev3)\n",
    "plt.xlabel(\"State\")\n",
    "plt.xticks(rotation=60)\n",
    "plt.ylabel(\"Number of Accidents\")\n",
    "plt.legend([\"Severity 1\", \"Severity 2\", \"Severity 3\", \"Severity 4\"])\n",
    "plt.title(\"Accident Severity by State (2016 - 2023)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart of accident severity by state without sev2 (sev 2 >> than the others so obscures sev 1, 3, 4 above)\n",
    "states = df_severity_by_state.columns\n",
    "sev1 = df_severity_by_state.iloc[0]\n",
    "sev3 = df_severity_by_state.iloc[2]\n",
    "sev4 = ct_sev4_by_state = df_severity_by_state.iloc[3]\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.bar(states, sev1)\n",
    "plt.bar(states, sev3)\n",
    "plt.bar(states, sev4)\n",
    "plt.xlabel(\"State\")\n",
    "plt.xticks(rotation=60)\n",
    "plt.ylabel(\"Number of Accidents\")\n",
    "plt.legend([\"Severity 1\", \"Severity 3\", \"Severity 4\"])\n",
    "plt.title(\"Accident Severity by State, Excluding Severity 2 (2016 - 2023)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sev_by_crossing = pd.crosstab(df_all_data['Severity'], df_all_data['Crossing'])\n",
    "df_sev_by_crossing = df_sev_by_crossing.rename(columns={False: \"No\", True: \"Yes\"})\n",
    "df_sev_by_crossing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# charts of accident severity by bump, traffic calming, roundabout\n",
    "# chart of accident severity by state\n",
    "crossing = df_sev_by_crossing.columns\n",
    "cr1 = df_sev_by_crossing.iloc[0]\n",
    "cr2 = df_sev_by_crossing.iloc[1]\n",
    "cr3 = df_sev_by_crossing.iloc[2]\n",
    "cr4 = df_sev_by_crossing.iloc[3]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.bar(crossing, cr1)\n",
    "plt.bar(crossing, cr2)\n",
    "plt.bar(crossing, cr3)\n",
    "plt.bar(crossing, cr4)\n",
    "plt.xticks(rotation=60)\n",
    "plt.xlabel(\"Nearby Crossing\")\n",
    "plt.ylabel(\"Number of Accidents\")\n",
    "plt.legend([\"Severity 1\", \"Severity 2\", \"Severity 3\", \"Severity 4\"])\n",
    "plt.title(\"Accident Severity by Proximity to Crossing, (2016 - 2023)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Count of Wind Direction Entries: {df_all_data['Wind_Direction'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Selection\n",
    "\n",
    "Based on my exploration of the data, I'm dropping the following fields from the dataset for the following reasons:\n",
    "\n",
    "- Source: contains information that has no relationship to causes and effects of accidents\n",
    "- Timezone: duplicates zip/state with less precision\n",
    "- Country: all data is from the United States so this field is redundant\n",
    "- Airport_Code: doesn't provide germane information-the exact location where weather conditions are reported is not a variable that can be adjusted\n",
    "- Weather_Timestamp: not related to the conditions of the accidents in any way\n",
    "- Wind_Direction: too many unique values; values are also not related to travel directions so it's unlikely they'll  produce clear/actionable conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Severity     Start_Lat     Start_Lng       End_Lat       End_Lng  \\\n",
      "count  7.728394e+06  7.728394e+06  7.728394e+06  4.325632e+06  4.325632e+06   \n",
      "mean   2.212384e+00  3.620119e+01 -9.470255e+01  3.626183e+01 -9.572557e+01   \n",
      "std    4.875313e-01  5.076079e+00  1.739176e+01  5.272905e+00  1.810793e+01   \n",
      "min    1.000000e+00  2.455480e+01 -1.246238e+02  2.456601e+01 -1.245457e+02   \n",
      "25%    2.000000e+00  3.339963e+01 -1.172194e+02  3.346207e+01 -1.177543e+02   \n",
      "50%    2.000000e+00  3.582397e+01 -8.776662e+01  3.618349e+01 -8.802789e+01   \n",
      "75%    2.000000e+00  4.008496e+01 -8.035368e+01  4.017892e+01 -8.024709e+01   \n",
      "max    4.000000e+00  4.900220e+01 -6.711317e+01  4.907500e+01 -6.710924e+01   \n",
      "\n",
      "       Distance(mi)  Temperature(F)  Wind_Chill(F)   Humidity(%)  \\\n",
      "count  7.728394e+06    7.564541e+06   5.729375e+06  7.554250e+06   \n",
      "mean   5.618423e-01    6.166329e+01   5.825105e+01  6.483104e+01   \n",
      "std    1.776811e+00    1.901365e+01   2.238983e+01  2.282097e+01   \n",
      "min    0.000000e+00   -8.900000e+01  -8.900000e+01  1.000000e+00   \n",
      "25%    0.000000e+00    4.900000e+01   4.300000e+01  4.800000e+01   \n",
      "50%    3.000000e-02    6.400000e+01   6.200000e+01  6.700000e+01   \n",
      "75%    4.640000e-01    7.600000e+01   7.500000e+01  8.400000e+01   \n",
      "max    4.417500e+02    2.070000e+02   2.070000e+02  1.000000e+02   \n",
      "\n",
      "       Pressure(in)  Visibility(mi)  Wind_Speed(mph)  Precipitation(in)  \n",
      "count  7.587715e+06    7.551296e+06     7.157161e+06       5.524808e+06  \n",
      "mean   2.953899e+01    9.090376e+00     7.685490e+00       8.407210e-03  \n",
      "std    1.006190e+00    2.688316e+00     5.424983e+00       1.102246e-01  \n",
      "min    0.000000e+00    0.000000e+00     0.000000e+00       0.000000e+00  \n",
      "25%    2.937000e+01    1.000000e+01     4.600000e+00       0.000000e+00  \n",
      "50%    2.986000e+01    1.000000e+01     7.000000e+00       0.000000e+00  \n",
      "75%    3.003000e+01    1.000000e+01     1.040000e+01       0.000000e+00  \n",
      "max    5.863000e+01    1.400000e+02     1.087000e+03       3.647000e+01  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7728394 entries, 0 to 7728393\n",
      "Data columns (total 40 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   ID                     object \n",
      " 1   Severity               int64  \n",
      " 2   Start_Time             object \n",
      " 3   End_Time               object \n",
      " 4   Start_Lat              float64\n",
      " 5   Start_Lng              float64\n",
      " 6   End_Lat                float64\n",
      " 7   End_Lng                float64\n",
      " 8   Distance(mi)           float64\n",
      " 9   Description            object \n",
      " 10  Street                 object \n",
      " 11  City                   object \n",
      " 12  County                 object \n",
      " 13  State                  object \n",
      " 14  Zipcode                object \n",
      " 15  Temperature(F)         float64\n",
      " 16  Wind_Chill(F)          float64\n",
      " 17  Humidity(%)            float64\n",
      " 18  Pressure(in)           float64\n",
      " 19  Visibility(mi)         float64\n",
      " 20  Wind_Speed(mph)        float64\n",
      " 21  Precipitation(in)      float64\n",
      " 22  Weather_Condition      object \n",
      " 23  Amenity                bool   \n",
      " 24  Bump                   bool   \n",
      " 25  Crossing               bool   \n",
      " 26  Give_Way               bool   \n",
      " 27  Junction               bool   \n",
      " 28  No_Exit                bool   \n",
      " 29  Railway                bool   \n",
      " 30  Roundabout             bool   \n",
      " 31  Station                bool   \n",
      " 32  Stop                   bool   \n",
      " 33  Traffic_Calming        bool   \n",
      " 34  Traffic_Signal         bool   \n",
      " 35  Turning_Loop           bool   \n",
      " 36  Sunrise_Sunset         object \n",
      " 37  Civil_Twilight         object \n",
      " 38  Nautical_Twilight      object \n",
      " 39  Astronomical_Twilight  object \n",
      "dtypes: bool(13), float64(12), int64(1), object(14)\n",
      "memory usage: 1.6+ GB\n",
      "None\n",
      "Index(['ID', 'Severity', 'Start_Time', 'End_Time', 'Start_Lat', 'Start_Lng',\n",
      "       'End_Lat', 'End_Lng', 'Distance(mi)', 'Description', 'Street', 'City',\n",
      "       'County', 'State', 'Zipcode', 'Temperature(F)', 'Wind_Chill(F)',\n",
      "       'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)',\n",
      "       'Precipitation(in)', 'Weather_Condition', 'Amenity', 'Bump', 'Crossing',\n",
      "       'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station',\n",
      "       'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop',\n",
      "       'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight',\n",
      "       'Astronomical_Twilight'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_refined = df_all_data.drop(['Source', 'Timezone', 'Country', 'Airport_Code', 'Weather_Timestamp', 'Wind_Direction'], axis=1)\n",
    "\n",
    "print(df_refined.describe())\n",
    "print(df_refined.info())\n",
    "print(df_refined.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Missing Values\n",
    "\n",
    "I've managed missing values in the code blocks below. Here's a brief explanation of my approch to each column:\n",
    "\n",
    "- Temperature, Wind_Chill, Humidity, Pressure, Visibility: imputed based on the mean temp of other accident entries sharing the same day and zip code (or state if there are none in the zip code)\n",
    "- Precipitation, Wind_Speed: assumed NaN indicates no precipitation/wind and replaced NaN with zero\n",
    "- Weather_Condition: consolidated entries from 144 to 30\n",
    "- Sunrise_Sunset: imputed based on\n",
    "- Civil_Twilight: imputed based on\n",
    "- Nautical_Twilight: imputed based on\n",
    "- Astronomical_Twilight: imputed based on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Severity</th>\n",
       "      <th>Start_Time</th>\n",
       "      <th>End_Time</th>\n",
       "      <th>Start_Lat</th>\n",
       "      <th>Start_Lng</th>\n",
       "      <th>End_Lat</th>\n",
       "      <th>End_Lng</th>\n",
       "      <th>Distance(mi)</th>\n",
       "      <th>Description</th>\n",
       "      <th>...</th>\n",
       "      <th>Station</th>\n",
       "      <th>Stop</th>\n",
       "      <th>Traffic_Calming</th>\n",
       "      <th>Traffic_Signal</th>\n",
       "      <th>Turning_Loop</th>\n",
       "      <th>Sunrise_Sunset</th>\n",
       "      <th>Civil_Twilight</th>\n",
       "      <th>Nautical_Twilight</th>\n",
       "      <th>Astronomical_Twilight</th>\n",
       "      <th>Acc_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7728389</th>\n",
       "      <td>A-7777757</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-08-23 18:03:25</td>\n",
       "      <td>2019-08-23 18:32:01</td>\n",
       "      <td>34.00248</td>\n",
       "      <td>-117.37936</td>\n",
       "      <td>33.99888</td>\n",
       "      <td>-117.37094</td>\n",
       "      <td>0.543</td>\n",
       "      <td>At Market St - Accident.</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>2019-08-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7728390</th>\n",
       "      <td>A-7777758</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-08-23 19:11:30</td>\n",
       "      <td>2019-08-23 19:38:23</td>\n",
       "      <td>32.76696</td>\n",
       "      <td>-117.14806</td>\n",
       "      <td>32.76555</td>\n",
       "      <td>-117.15363</td>\n",
       "      <td>0.338</td>\n",
       "      <td>At Camino Del Rio/Mission Center Rd - Accident.</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>2019-08-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7728391</th>\n",
       "      <td>A-7777759</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-08-23 19:00:21</td>\n",
       "      <td>2019-08-23 19:28:49</td>\n",
       "      <td>33.77545</td>\n",
       "      <td>-117.84779</td>\n",
       "      <td>33.77740</td>\n",
       "      <td>-117.85727</td>\n",
       "      <td>0.561</td>\n",
       "      <td>At Glassell St/Grand Ave - Accident. in the ri...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>2019-08-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7728392</th>\n",
       "      <td>A-7777760</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-08-23 19:00:21</td>\n",
       "      <td>2019-08-23 19:29:42</td>\n",
       "      <td>33.99246</td>\n",
       "      <td>-118.40302</td>\n",
       "      <td>33.98311</td>\n",
       "      <td>-118.39565</td>\n",
       "      <td>0.772</td>\n",
       "      <td>At CA-90/Marina Fwy/Jefferson Blvd - Accident.</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>2019-08-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7728393</th>\n",
       "      <td>A-7777761</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-08-23 18:52:06</td>\n",
       "      <td>2019-08-23 19:21:31</td>\n",
       "      <td>34.13393</td>\n",
       "      <td>-117.23092</td>\n",
       "      <td>34.13736</td>\n",
       "      <td>-117.23934</td>\n",
       "      <td>0.537</td>\n",
       "      <td>At Highland Ave/Arden Ave - Accident.</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>Day</td>\n",
       "      <td>2019-08-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                ID  Severity          Start_Time            End_Time  \\\n",
       "7728389  A-7777757         2 2019-08-23 18:03:25 2019-08-23 18:32:01   \n",
       "7728390  A-7777758         2 2019-08-23 19:11:30 2019-08-23 19:38:23   \n",
       "7728391  A-7777759         2 2019-08-23 19:00:21 2019-08-23 19:28:49   \n",
       "7728392  A-7777760         2 2019-08-23 19:00:21 2019-08-23 19:29:42   \n",
       "7728393  A-7777761         2 2019-08-23 18:52:06 2019-08-23 19:21:31   \n",
       "\n",
       "         Start_Lat  Start_Lng   End_Lat    End_Lng  Distance(mi)  \\\n",
       "7728389   34.00248 -117.37936  33.99888 -117.37094         0.543   \n",
       "7728390   32.76696 -117.14806  32.76555 -117.15363         0.338   \n",
       "7728391   33.77545 -117.84779  33.77740 -117.85727         0.561   \n",
       "7728392   33.99246 -118.40302  33.98311 -118.39565         0.772   \n",
       "7728393   34.13393 -117.23092  34.13736 -117.23934         0.537   \n",
       "\n",
       "                                               Description  ... Station  \\\n",
       "7728389                           At Market St - Accident.  ...   False   \n",
       "7728390    At Camino Del Rio/Mission Center Rd - Accident.  ...   False   \n",
       "7728391  At Glassell St/Grand Ave - Accident. in the ri...  ...   False   \n",
       "7728392     At CA-90/Marina Fwy/Jefferson Blvd - Accident.  ...   False   \n",
       "7728393              At Highland Ave/Arden Ave - Accident.  ...   False   \n",
       "\n",
       "          Stop Traffic_Calming Traffic_Signal Turning_Loop  Sunrise_Sunset  \\\n",
       "7728389  False           False          False        False             Day   \n",
       "7728390  False           False          False        False             Day   \n",
       "7728391  False           False          False        False             Day   \n",
       "7728392  False           False          False        False             Day   \n",
       "7728393  False           False          False        False             Day   \n",
       "\n",
       "         Civil_Twilight  Nautical_Twilight  Astronomical_Twilight    Acc_date  \n",
       "7728389             Day                Day                    Day  2019-08-23  \n",
       "7728390             Day                Day                    Day  2019-08-23  \n",
       "7728391             Day                Day                    Day  2019-08-23  \n",
       "7728392             Day                Day                    Day  2019-08-23  \n",
       "7728393             Day                Day                    Day  2019-08-23  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_refined['Start_Time'] = pd.to_datetime(df_refined['Start_Time'], yearfirst=True, format='mixed')\n",
    "df_refined['End_Time'] = pd.to_datetime(df_refined['End_Time'], yearfirst=True, format='mixed')\n",
    "df_refined['Acc_date'] = df_refined['Start_Time'].dt.date\n",
    "df_refined.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing_data(input_df, date='Acc_date', zip='Zipcode', state='State', temp='Temperature(F)', \n",
    "                        windchill='Wind_Chill(F)', hum='Humidity(%)', press='Pressure(in)', vis='Visibility(mi)'):\n",
    "    \"\"\"\n",
    "    Replaces NaN values with the mean values of entries\n",
    "    with the same date and zip code. If no match exists with date and county,\n",
    "    uses date and state.\n",
    "    \"\"\"\n",
    "\n",
    "    def fill_missing_temperatures(input_df, date='date', zip='zip', state='state', temp='temp'):\n",
    "        df_filled = df_refined.copy()\n",
    "        \n",
    "        # Step 1: mean temperature for each date-zip combination\n",
    "        temp_means_zip = df_filled.groupby([date, zip])[temp].transform('mean')\n",
    "        \n",
    "        # fill NaN values with the date-zip group mean\n",
    "        df_filled[temp] = df_filled[temp].fillna(temp_means_zip)\n",
    "        \n",
    "        # Step 2: date-state combination for remaining NaNs\n",
    "        if df_filled[temp].isna().any():\n",
    "            remaining = df_filled[temp].isna().sum()\n",
    "            print(f\"Info: {remaining} temperatures still missing after date-zip fill.\")\n",
    "            print(\"Filling remaining with date-state mean.\")\n",
    "            \n",
    "            temp_means_state = df_filled.groupby([date, state])[temp].transform('mean')\n",
    "            df_filled[temp] = df_filled[temp].fillna(temp_means_state)\n",
    "        \n",
    "        # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "        if df_filled[temp].isna().any():\n",
    "            remaining = df_filled[temp].isna().sum()\n",
    "            print(f\"Warning: {remaining} temperatures still missing after date-state fill.\")\n",
    "            print(\"Filling remaining with overall mean as last resort.\")\n",
    "            df_filled[temp] = df_filled[temp].fillna(df_filled[temp].mean())\n",
    "        \n",
    "        return df_filled\n",
    "\n",
    "    def fill_missing_windchill(df='df_filled', date='date', zip='zip', state='state', windchill='windchill'):\n",
    "        # Step 1: mean windchill for each date-zip combination\n",
    "        windchill_means_zip = df_filled.groupby([date, zip])[windchill].transform('mean')\n",
    "        \n",
    "        # fill NaN values with the date-zip group mean\n",
    "        df_filled[windchill] = df_filled[windchill].fillna(windchill_means_zip)\n",
    "        \n",
    "        # Step 2: date-state combination for remaining NaNs\n",
    "        if df_filled[windchill].isna().any():\n",
    "            remaining = df_filled[windchill].isna().sum()\n",
    "            print(f\"Info: {remaining} wind chills still missing after date-zip fill.\")\n",
    "            print(\"Filling remaining with date-state mean.\")\n",
    "            \n",
    "            windchill_means_state = df_filled.groupby([date, state])[windchill].transform('mean')\n",
    "            df_filled[windchill] = df_filled[windchill].fillna(windchill_means_state)\n",
    "        \n",
    "        # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "        if df_filled[windchill].isna().any():\n",
    "            remaining = df_filled[windchill].isna().sum()\n",
    "            print(f\"Warning: {remaining} wind chills still missing after date-state fill.\")\n",
    "            print(\"Filling remaining with overall mean as last resort.\")\n",
    "            df_filled[windchill] = df_filled[windchill].fillna(df_filled[windchill].mean())\n",
    "        \n",
    "        return df_filled\n",
    "    \n",
    "    def fill_missing_press(df='df_filled', date='date', zip='zip', state='state', press='press'):\n",
    "        # Step 1: mean pressure for each date-zip combination\n",
    "        press_means_zip = df_filled.groupby([date, zip])[press].transform('mean')\n",
    "        \n",
    "        # fill NaN values with the date-zip group mean\n",
    "        df_filled[press] = df_filled[press].fillna(press_means_zip)\n",
    "        \n",
    "        # Step 2: date-state combination for remaining NaNs\n",
    "        if df_filled[press].isna().any():\n",
    "            remaining = df_filled[press].isna().sum()\n",
    "            print(f\"Info: {remaining} pressure values still missing after date-zip fill.\")\n",
    "            print(\"Filling remaining with date-state mean.\")\n",
    "            \n",
    "            press_means_state = df_filled.groupby([date, state])[press].transform('mean')\n",
    "            df_filled[press] = df_filled[press].fillna(hum_means_state)\n",
    "        \n",
    "        # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "        if df_filled[press].isna().any():\n",
    "            remaining = df_filled[press].isna().sum()\n",
    "            print(f\"Warning: {remaining} pressure values still missing after date-state fill.\")\n",
    "            print(\"Filling remaining with overall mean as last resort.\")\n",
    "            df_filled[press] = df_filled[press].fillna(df_filled[press].mean())\n",
    "        \n",
    "        return df_filled\n",
    "    \n",
    "    def fill_missing_vis(df='df_filled', date='date', zip='zip', state='state', vis='vis'):\n",
    "        # Step 1: mean pressure for each date-zip combination\n",
    "        vis_means_zip = df_filled.groupby([date, zip])[vis].transform('mean')\n",
    "        \n",
    "        # fill NaN values with the date-zip group mean\n",
    "        df_filled[vis] = df_filled[vis].fillna(vis_means_zip)\n",
    "        \n",
    "        # Step 2: date-state combination for remaining NaNs\n",
    "        if df_filled[vis].isna().any():\n",
    "            remaining = df_filled[vis].isna().sum()\n",
    "            print(f\"Info: {remaining} visibility values still missing after date-zip fill.\")\n",
    "            print(\"Filling remaining with date-state mean.\")\n",
    "            \n",
    "            vis_means_state = df_filled.groupby([date, state])[vis].transform('mean')\n",
    "            df_filled[vis] = df_filled[vis].fillna(vis_means_state)\n",
    "        \n",
    "        # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "        if df_filled[vis].isna().any():\n",
    "            remaining = df_filled[vis].isna().sum()\n",
    "            print(f\"Warning: {remaining} visibility values still missing after date-state fill.\")\n",
    "            print(\"Filling remaining with overall mean as last resort.\")\n",
    "            df_filled[vis] = df_filled[vis].fillna(df_filled[vis].mean())\n",
    "        \n",
    "        return df_filled\n",
    "    \n",
    "    return df_filled\n",
    "    \n",
    "\n",
    "df_filled = impute_missing_data(df_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing NaN values in the temperature column\n",
    "\n",
    "def fill_missing_temperatures(df, date_col='Acc_date', zip_col='Zipcode', state_col='State', temp_col='Temperature(F)'):\n",
    "    \"\"\"\n",
    "    Replaces NaN Temperature values with the mean temperature of entries\n",
    "    with the same date and zip code. If no match exists with date and county,\n",
    "    uses date and state.\n",
    "    \"\"\"\n",
    "    # Step 1: mean temperature for each date-zip combination\n",
    "    df_temp_fill = df_refined.copy()\n",
    "    \n",
    "    temp_means_zip = df_temp_fill.groupby([date_col, zip_col])[temp_col].transform('mean')\n",
    "    \n",
    "    # fill NaN values with the date-zip group mean\n",
    "    df_temp_fill[temp_col] = df_temp_fill[temp_col].fillna(temp_means_zip)\n",
    "    \n",
    "    # Step 2: date-state combination for remaining NaNs\n",
    "    if df_temp_fill[temp_col].isna().any():\n",
    "        remaining = df_temp_fill[temp_col].isna().sum()\n",
    "        print(f\"Info: {remaining} temperatures still missing after date-zip fill.\")\n",
    "        print(\"Filling remaining with date-state mean.\")\n",
    "        \n",
    "        temp_means_state = df_temp_fill.groupby([date_col, state_col])[temp_col].transform('mean')\n",
    "        df_temp_fill[temp_col] = df_temp_fill[temp_col].fillna(temp_means_state)\n",
    "    \n",
    "    # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "    if df_temp_fill[temp_col].isna().any():\n",
    "        remaining = df_temp_fill[temp_col].isna().sum()\n",
    "        print(f\"Warning: {remaining} temperatures still missing after date-state fill.\")\n",
    "        print(\"Filling remaining with overall mean as last resort.\")\n",
    "        df_temp_fill[temp_col] = df_temp_fill[temp_col].fillna(df_temp_fill[temp_col].mean())\n",
    "    \n",
    "    return df_temp_fill\n",
    "\n",
    "\n",
    "\n",
    "df_temp_fill = fill_missing_temperatures(df_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: 158386 temperature entries still missing after date-zip fill.\n",
      "Filling remaining with date-state mean.\n",
      "Warning: 7341 temperature entries still missing after date-state fill.\n",
      "Filling remaining with overall mean as last resort.\n"
     ]
    }
   ],
   "source": [
    "# replacing NaN values in the temperature column\n",
    "\n",
    "def fill_missing_temp(df, date='Acc_date', zip='Zipcode', state='State', temp='Temperature(F)'):\n",
    "    \"\"\"\n",
    "    Replaces NaN humidity values with the mean humidity of entries\n",
    "    with the same date and zip code. If no match exists with date and county,\n",
    "    uses date and state.\n",
    "    \"\"\"\n",
    "    # Step 1: mean humidity for each date-zip combination\n",
    "    df_temp_fill = df_refined.copy()\n",
    "    \n",
    "    temp_means_zip = df_temp_fill.groupby([date, zip])[temp].transform('mean')\n",
    "    \n",
    "    # fill NaN values with the date-zip group mean\n",
    "    df_temp_fill[temp] = df_temp_fill[temp].fillna(temp_means_zip)\n",
    "    \n",
    "    # Step 2: date-state combination for remaining NaNs\n",
    "    if df_temp_fill[temp].isna().any():\n",
    "        remaining = df_temp_fill[temp].isna().sum()\n",
    "        print(f\"Info: {remaining} temperature entries still missing after date-zip fill.\")\n",
    "        print(\"Filling remaining with date-state mean.\")\n",
    "        \n",
    "        temp_means_state = df_temp_fill.groupby([date, state])[temp].transform('mean')\n",
    "        df_temp_fill[temp] = df_temp_fill[temp].fillna(temp_means_state)\n",
    "    \n",
    "    # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "    if df_temp_fill[temp].isna().any():\n",
    "        remaining = df_temp_fill[temp].isna().sum()\n",
    "        print(f\"Warning: {remaining} temperature entries still missing after date-state fill.\")\n",
    "        print(\"Filling remaining with overall mean as last resort.\")\n",
    "        df_temp_fill[temp] = df_temp_fill[temp].fillna(df_temp_fill[temp].mean())\n",
    "    \n",
    "    return df_temp_fill\n",
    "\n",
    "df_temp_fill = fill_missing_temp(df_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying NaNs have been replaced\n",
    "df_temp_fill['Temperature(F)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with NaN Temperature: 0\n"
     ]
    }
   ],
   "source": [
    "# writing NaN-free column back to the original dataframe\n",
    "df_refined['Temperature(F)'] = df_temp_fill['Temperature(F)']\n",
    "print(f\"Entries with NaN Temperature: {df_refined['Temperature(F)'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1031"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deleting working df to free up memory\n",
    "del df_temp_fill\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: 1949795 wind chills still missing after date-zip fill.\n",
      "Filling remaining with date-state mean.\n",
      "Warning: 687931 wind chills still missing after date-state fill.\n",
      "Filling remaining with overall mean as last resort.\n"
     ]
    }
   ],
   "source": [
    "# replacing NaN values in the wind chill column\n",
    "\n",
    "def fill_missing_windchill(df, date='Acc_date', zip='Zipcode', state='State', windchill='Wind_Chill(F)'):\n",
    "    \"\"\"\n",
    "    Replaces NaN wind chill values with the mean wind chill of entries\n",
    "    with the same date and zip code. If no match exists with date and county,\n",
    "    uses date and state.\n",
    "    \"\"\"\n",
    "    # Step 1: mean temperature for each date-zip combination\n",
    "    df_windchill_fill = df_refined.copy()\n",
    "    \n",
    "    windchill_means_zip = df_windchill_fill.groupby([date, zip])[windchill].transform('mean')\n",
    "    \n",
    "    # fill NaN values with the date-zip group mean\n",
    "    df_windchill_fill[windchill] = df_windchill_fill[windchill].fillna(windchill_means_zip)\n",
    "    \n",
    "    # Step 2: date-state combination for remaining NaNs\n",
    "    if df_windchill_fill[windchill].isna().any():\n",
    "        remaining = df_windchill_fill[windchill].isna().sum()\n",
    "        print(f\"Info: {remaining} wind chills still missing after date-zip fill.\")\n",
    "        print(\"Filling remaining with date-state mean.\")\n",
    "        \n",
    "        windchill_means_state = df_windchill_fill.groupby([date, state])[windchill].transform('mean')\n",
    "        df_windchill_fill[windchill] = df_windchill_fill[windchill].fillna(windchill_means_state)\n",
    "    \n",
    "    # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "    if df_windchill_fill[windchill].isna().any():\n",
    "        remaining = df_windchill_fill[windchill].isna().sum()\n",
    "        print(f\"Warning: {remaining} wind chills still missing after date-state fill.\")\n",
    "        print(\"Filling remaining with overall mean as last resort.\")\n",
    "        df_windchill_fill[windchill] = df_windchill_fill[windchill].fillna(df_windchill_fill[windchill].mean())\n",
    "    \n",
    "    return df_windchill_fill\n",
    "\n",
    "df_windchill_fill = fill_missing_windchill(df_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying NaNs have been replaced in the working df\n",
    "df_windchill_fill['Wind_Chill(F)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with NaN Wind Chill: 0\n"
     ]
    }
   ],
   "source": [
    "# writing NaN-free column back to the original dataframe\n",
    "df_refined['Wind_Chill(F)'] = df_windchill_fill['Wind_Chill(F)']\n",
    "print(f\"Entries with NaN Wind Chill: {df_refined['Wind_Chill(F)'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deleting working df to free up memory\n",
    "del df_windchill_fill\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: 166329 humidity entries still missing after date-zip fill.\n",
      "Filling remaining with date-state mean.\n",
      "Warning: 7358 humidity entries still missing after date-state fill.\n",
      "Filling remaining with overall mean as last resort.\n"
     ]
    }
   ],
   "source": [
    "# replacing NaN values in the humidity column\n",
    "\n",
    "def fill_missing_hum(df, date='Acc_date', zip='Zipcode', state='State', hum='Humidity(%)'):\n",
    "    \"\"\"\n",
    "    Replaces NaN humidity values with the mean humidity of entries\n",
    "    with the same date and zip code. If no match exists with date and county,\n",
    "    uses date and state.\n",
    "    \"\"\"\n",
    "    # Step 1: mean humidity for each date-zip combination\n",
    "    df_hum_fill = df_refined.copy()\n",
    "    \n",
    "    hum_means_zip = df_hum_fill.groupby([date, zip])[hum].transform('mean')\n",
    "    \n",
    "    # fill NaN values with the date-zip group mean\n",
    "    df_hum_fill[hum] = df_hum_fill[hum].fillna(hum_means_zip)\n",
    "    \n",
    "    # Step 2: date-state combination for remaining NaNs\n",
    "    if df_hum_fill[hum].isna().any():\n",
    "        remaining = df_hum_fill[hum].isna().sum()\n",
    "        print(f\"Info: {remaining} humidity entries still missing after date-zip fill.\")\n",
    "        print(\"Filling remaining with date-state mean.\")\n",
    "        \n",
    "        hum_means_state = df_hum_fill.groupby([date, state])[hum].transform('mean')\n",
    "        df_hum_fill[hum] = df_hum_fill[hum].fillna(hum_means_state)\n",
    "    \n",
    "    # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "    if df_hum_fill[hum].isna().any():\n",
    "        remaining = df_hum_fill[hum].isna().sum()\n",
    "        print(f\"Warning: {remaining} humidity entries still missing after date-state fill.\")\n",
    "        print(\"Filling remaining with overall mean as last resort.\")\n",
    "        df_hum_fill[hum] = df_hum_fill[hum].fillna(df_hum_fill[hum].mean())\n",
    "    \n",
    "    return df_hum_fill\n",
    "\n",
    "df_hum_fill = fill_missing_hum(df_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying NaNs have been replaced in the working df\n",
    "df_hum_fill['Humidity(%)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with NaN humidity: 0\n"
     ]
    }
   ],
   "source": [
    "# writing NaN-free column back to the original dataframe\n",
    "df_refined['Humidity(%)'] = df_hum_fill['Humidity(%)']\n",
    "print(f\"Entries with NaN humidity: {df_refined['Humidity(%)'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deleting working df to free up memory\n",
    "del df_hum_fill\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: 137561 pressure entries still missing after date-zip fill.\n",
      "Filling remaining with date-state mean.\n",
      "Warning: 7313 pressure entries still missing after date-state fill.\n",
      "Filling remaining with overall mean as last resort.\n"
     ]
    }
   ],
   "source": [
    "# replacing NaN values in the pressure column\n",
    "\n",
    "def fill_missing_press(df, date='Acc_date', zip='Zipcode', state='State', press='Pressure(in)'):\n",
    "    \"\"\"\n",
    "    Replaces NaN pressure values with the mean pressure of entries\n",
    "    with the same date and zip code. If no match exists with date and county,\n",
    "    uses date and state.\n",
    "    \"\"\"\n",
    "    # Step 1: mean pressure for each date-zip combination\n",
    "    df_press_fill = df_refined.copy()\n",
    "    \n",
    "    press_means_zip = df_press_fill.groupby([date, zip])[press].transform('mean')\n",
    "    \n",
    "    # fill NaN values with the date-zip group mean\n",
    "    df_press_fill[press] = df_press_fill[press].fillna(press_means_zip)\n",
    "    \n",
    "    # Step 2: date-state combination for remaining NaNs\n",
    "    if df_press_fill[press].isna().any():\n",
    "        remaining = df_press_fill[press].isna().sum()\n",
    "        print(f\"Info: {remaining} pressure entries still missing after date-zip fill.\")\n",
    "        print(\"Filling remaining with date-state mean.\")\n",
    "        \n",
    "        press_means_state = df_press_fill.groupby([date, state])[press].transform('mean')\n",
    "        df_press_fill[press] = df_hum_fill[press].fillna(press_means_state)\n",
    "    \n",
    "    # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "    if df_press_fill[press].isna().any():\n",
    "        remaining = df_press_fill[press].isna().sum()\n",
    "        print(f\"Warning: {remaining} pressure entries still missing after date-state fill.\")\n",
    "        print(\"Filling remaining with overall mean as last resort.\")\n",
    "        df_press_fill[press] = df_press_fill[press].fillna(df_press_fill[press].mean())\n",
    "    \n",
    "    return df_press_fill\n",
    "\n",
    "df_press_fill = fill_missing_press(df_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying NaNs have been replaced in the working df\n",
    "df_press_fill['Pressure(in)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with NaN pressure: 0\n"
     ]
    }
   ],
   "source": [
    "# writing NaN-free column back to the original dataframe\n",
    "df_refined['Pressure(in)'] = df_press_fill['Pressure(in)']\n",
    "print(f\"Entries with NaN pressure: {df_refined['Pressure(in)'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deleting working df to free up memory\n",
    "del df_press_fill\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: 171980 visibility entries still missing after date-zip fill.\n",
      "Filling remaining with date-state mean.\n",
      "Warning: 7409 visibility entries still missing after date-state fill.\n",
      "Filling remaining with overall mean as last resort.\n"
     ]
    }
   ],
   "source": [
    "# replacing NaN values in the visibility column\n",
    "\n",
    "def fill_missing_vis(df, date='Acc_date', zip='Zipcode', state='State', vis='Visibility(mi)'):\n",
    "    \"\"\"\n",
    "    Replaces NaN visibility values with the mean visibility of entries\n",
    "    with the same date and zip code. If no match exists with date and county,\n",
    "    uses date and state.\n",
    "    \"\"\"\n",
    "    # Step 1: mean visibility for each date-zip combination\n",
    "    df_vis_fill = df_refined.copy()\n",
    "    \n",
    "    vis_means_zip = df_vis_fill.groupby([date, zip])[vis].transform('mean')\n",
    "    \n",
    "    # fill NaN values with the date-zip group mean\n",
    "    df_vis_fill[vis] = df_vis_fill[vis].fillna(vis_means_zip)\n",
    "    \n",
    "    # Step 2: date-state combination for remaining NaNs\n",
    "    if df_vis_fill[vis].isna().any():\n",
    "        remaining = df_vis_fill[vis].isna().sum()\n",
    "        print(f\"Info: {remaining} visibility entries still missing after date-zip fill.\")\n",
    "        print(\"Filling remaining with date-state mean.\")\n",
    "        \n",
    "        vis_means_state = df_vis_fill.groupby([date, state])[vis].transform('mean')\n",
    "        df_vis_fill[vis] = df_hum_fill[vis].fillna(vis_means_state)\n",
    "    \n",
    "    # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "    if df_vis_fill[vis].isna().any():\n",
    "        remaining = df_vis_fill[vis].isna().sum()\n",
    "        print(f\"Warning: {remaining} visibility entries still missing after date-state fill.\")\n",
    "        print(\"Filling remaining with overall mean as last resort.\")\n",
    "        df_vis_fill[vis] = df_vis_fill[vis].fillna(df_vis_fill[vis].mean())\n",
    "    \n",
    "    return df_vis_fill\n",
    "\n",
    "df_vis_fill = fill_missing_vis(df_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying NaNs have been replaced in the working df\n",
    "df_vis_fill['Visibility(mi)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with NaN visibility: 0\n"
     ]
    }
   ],
   "source": [
    "# writing NaN-free column back to the original dataframe\n",
    "df_refined['Visibility(mi)'] = df_vis_fill['Visibility(mi)']\n",
    "print(f\"Entries with NaN visibility: {df_refined['Visibility(mi)'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deleting working df to free up memory\n",
    "del df_vis_fill\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with NaN Wind Speed: 0\n",
      "Entries with NaN Precipitation: 0\n"
     ]
    }
   ],
   "source": [
    "# replacing NaN with 0 in wind speed and precipitation\n",
    "\n",
    "df_refined['Wind_Speed(mph)'] = df_refined['Wind_Speed(mph)'].fillna(0)\n",
    "df_refined['Precipitation(in)'] = df_refined['Precipitation(in)'].fillna(0)\n",
    "print(f\"Entries with NaN Wind Speed: {df_refined['Wind_Speed(mph)'].isna().sum()}\")\n",
    "print(f\"Entries with NaN Precipitation: {df_refined['Precipitation(in)'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Count of Unique Weather Condition Entries: {df_all_data['Weather_Condition'].nunique()}\")\n",
    "df_refined['Weather_Condition'].value_counts().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing a list of weather condition unique values to csv\n",
    "unique_wthr_cond = pd.Series(df_refined['Weather_Condition'].unique())\n",
    "unique_wthr_cond.to_csv('conditions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_test\n",
    "del conditions_mapper\n",
    "del mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = {'Blowing Dust': 'Blowing Ash / Dust / Sand',\n",
    "  'Blowing Dust / Windy': 'Blowing Ash / Dust / Sand',\n",
    "  'Blowing Sand': 'Blowing Ash / Dust / Sand',\n",
    "  'Blowing Snow': 'Blowing Snow',\n",
    "  'Blowing Snow / Windy': 'Blowing Snow',\n",
    "  'Blowing Snow Nearby': 'Blowing Snow',\n",
    "  'Clear': 'Fair',\n",
    "  'Cloudy': 'Cloudy',\n",
    "  'Cloudy / Windy': 'Cloudy',\n",
    "  'Drifting Snow': 'Blowing Snow',\n",
    "  'Drifting Snow / Windy': 'Blowing Snow',\n",
    "  'Drizzle': 'Light Rain',\n",
    "  'Drizzle / Windy': 'Light Rain',\n",
    "  'Drizzle and Fog': 'Light Rain',\n",
    "  'Dust Whirls': 'Blowing Ash / Dust / Sand',\n",
    "  'Duststorm': 'Blowing Ash / Dust / Sand',\n",
    "  'Fair': 'Fair',\n",
    "  'Fair / Windy': 'Fair',\n",
    "  'Fog': 'Fog',\n",
    "  'Fog / Windy': 'Fog',\n",
    "  'Freezing Drizzle': 'Freezing Rain',\n",
    "  'Freezing Rain': 'Freezing Rain',\n",
    "  'Freezing Rain / Windy': 'Freezing Rain',\n",
    "  'Funnel Cloud': 'Tornado',\n",
    "  'Hail': 'Hail',\n",
    "  'Haze': 'Hazy',\n",
    "  'Haze / Windy': 'Hazy',\n",
    "  'Heavy Blowing Snow': 'Heavy Snow',\n",
    "  'Heavy Drizzle': 'Rain',\n",
    "  'Heavy Freezing Drizzle': 'Freezing Rain',\n",
    "  'Heavy Freezing Rain': 'Heavy Freezing Rain',\n",
    "  'Heavy Freezing Rain / Windy': 'Heavy Freezing Rain',\n",
    "  'Heavy Ice Pellets': 'Heavy Sleet',\n",
    "  'Heavy Rain': 'Heavy Rain',\n",
    "  'Heavy Rain / Windy': 'Heavy Rain',\n",
    "  'Heavy Rain Shower': 'Heavy Rain',\n",
    "  'Heavy Rain Shower / Windy': 'Heavy Rain',\n",
    "  'Heavy Rain Showers': 'Heavy Rain',\n",
    "  'Heavy Sleet': 'Heavy Sleet',\n",
    "  'Heavy Sleet / Windy': 'Heavy Sleet',\n",
    "  'Heavy Sleet and Thunder': 'Heavy Sleet',\n",
    "  'Heavy Smoke': 'Heavy Smoke',\n",
    "  'Heavy Snow': 'Heavy Snow',\n",
    "  'Heavy Snow / Windy': 'Heavy Snow',\n",
    "  'Heavy Snow with Thunder': 'Heavy Snow',\n",
    "  'Heavy Thunderstorms and Rain': 'Heavy Thunderstorm',\n",
    "  'Heavy Thunderstorms and Snow': 'Heavy Snow',\n",
    "  'Heavy Thunderstorms with Small Hail': 'Heavy Thunderstorm',\n",
    "  'Heavy T-Storm': 'Heavy Thunderstorm',\n",
    "  'Heavy T-Storm / Windy': 'Heavy Thunderstorm',\n",
    "  'Ice Pellets': 'Sleet',\n",
    "  'Light Blowing Snow': 'Light Snow',\n",
    "  'Light Drizzle': 'Light Rain',\n",
    "  'Light Drizzle / Windy': 'Light Rain',\n",
    "  'Light Fog': 'Light Fog',\n",
    "  'Light Freezing Drizzle': 'Light Sleet',\n",
    "  'Light Freezing Fog': 'Light Freezing Fog',\n",
    "  'Light Freezing Rain': 'Light Sleet',\n",
    "  'Light Freezing Rain / Windy': 'Light Sleet',\n",
    "  'Light Hail': 'Light Hail',\n",
    "  'Light Haze': 'Hazy',\n",
    "  'Light Ice Pellets': 'Light Sleet',\n",
    "  'Light Rain': 'Light Rain',\n",
    "  'Light Rain / Windy': 'Light Rain',\n",
    "  'Light Rain Shower': 'Light Rain',\n",
    "  'Light Rain Shower / Windy': 'Light Rain',\n",
    "  'Light Rain Showers': 'Light Rain',\n",
    "  'Light Rain with Thunder': 'Light Rain',\n",
    "  'Light Sleet': 'Light Sleet',\n",
    "  'Light Sleet / Windy': 'Light Sleet',\n",
    "  'Light Snow': 'Light Snow',\n",
    "  'Light Snow / Windy': 'Light Snow',\n",
    "  'Light Snow and Sleet': 'Wintry  Mix',\n",
    "  'Light Snow and Sleet / Windy': 'Wintry  Mix',\n",
    "  'Light Snow Grains': 'Light Snow',\n",
    "  'Light Snow Shower': 'Light Snow',\n",
    "  'Light Snow Shower / Windy': 'Light Snow',\n",
    "  'Light Snow Showers': 'Light Snow',\n",
    "  'Light Snow with Thunder': 'Light Snow',\n",
    "  'Light Thunderstorm': 'Light Thunderstorm',\n",
    "  'Light Thunderstorms and Rain': 'Light Thunderstorm',\n",
    "  'Light Thunderstorms and Snow': 'Light Snow',\n",
    "  'Low Drifting Snow': 'Light Snow',\n",
    "  'Mist': 'Mist',\n",
    "  'Mist / Windy': 'Mist',\n",
    "  'Mostly Cloudy': 'Cloudy',\n",
    "  'Mostly Cloudy / Windy': 'Cloudy',\n",
    "  'N/A Precipitation': 'Fair',\n",
    "  'Overcast': 'Cloudy',\n",
    "  'Partial Fog': 'Light Fog',\n",
    "  'Partial Fog / Windy': 'Light Fog',\n",
    "  'Partly Cloudy': 'Partly Cloudy',\n",
    "  'Partly Cloudy / Windy': 'Partly Cloudy',\n",
    "  'Patches of Fog': 'Light Fog',\n",
    "  'Patches of Fog / Windy': 'Light Fog',\n",
    "  'Rain': 'Rain',\n",
    "  'Rain / Windy': 'Rain',\n",
    "  'Rain and Sleet': 'Sleet',\n",
    "  'Rain Shower': 'Rain',\n",
    "  'Rain Shower / Windy': 'Rain',\n",
    "  'Rain Showers': 'Rain',\n",
    "  'Sand': 'Blowing Ash / Dust / Sand',\n",
    "  'Sand / Dust Whirls Nearby': 'Blowing Ash / Dust / Sand',\n",
    "  'Sand / Dust Whirlwinds': 'Blowing Ash / Dust / Sand',\n",
    "  'Sand / Dust Whirlwinds / Windy': 'Blowing Ash / Dust / Sand',\n",
    "  'Sand / Windy': 'Blowing Ash / Dust / Sand',\n",
    "  'Scattered Clouds': 'Partly Cloudy',\n",
    "  'Shallow Fog': 'Light Fog',\n",
    "  'Shallow Fog / Windy': 'Light Fog',\n",
    "  'Showers in the Vicinity': 'Partly Cloudy',\n",
    "  'Sleet': 'Sleet',\n",
    "  'Sleet / Windy': 'Sleet',\n",
    "  'Sleet and Thunder': 'Sleet',\n",
    "  'Small Hail': 'Hail',\n",
    "  'Smoke': 'Smoke',\n",
    "  'Smoke / Windy': 'Smoke',\n",
    "  'Snow': 'Snow',\n",
    "  'Snow / Windy': 'Snow',\n",
    "  'Snow and Sleet': 'Wintry  Mix',\n",
    "  'Snow and Sleet / Windy': 'Wintry  Mix',\n",
    "  'Snow and Thunder': 'Snow',\n",
    "  'Snow and Thunder / Windy': 'Snow',\n",
    "  'Snow Grains': 'Light Snow',\n",
    "  'Snow Showers': 'Snow',\n",
    "  'Squalls': 'Thunderstorm',\n",
    "  'Squalls / Windy': 'Thunderstorm',\n",
    "  'Thunder': 'Thunderstorm',\n",
    "  'Thunder / Windy': 'Thunderstorm',\n",
    "  'Thunder / Wintry Mix': 'Wintry  Mix',\n",
    "  'Thunder / Wintry Mix / Windy': 'Wintry  Mix',\n",
    "  'Thunder and Hail': 'Hail',\n",
    "  'Thunder and Hail / Windy': 'Hail',\n",
    "  'Thunder in the Vicinity': 'Light Thunderstorm',\n",
    "  'Thunderstorm': 'Thunderstorm',\n",
    "  'Thunderstorms and Rain': 'Thunderstorm',\n",
    "  'Thunderstorms and Snow': 'Snow',\n",
    "  'Tornado': 'Tornado',\n",
    "  'T-Storm': 'Tornado',\n",
    "  'T-Storm / Windy': 'Tornado',\n",
    "  'Volcanic Ash': 'Blowing Ash / Dust / Sand',\n",
    "  'Widespread Dust': 'Blowing Ash / Dust / Sand',\n",
    "  'Widespread Dust / Windy': 'Blowing Ash / Dust / Sand',\n",
    "  'Wintry Mix': 'Wintry  Mix',\n",
    "  'Wintry Mix / Windy': 'Wintry  Mix'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 767. MiB for an array with shape (13, 7728394) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_test \u001b[38;5;241m=\u001b[39m \u001b[43mdf_refined\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeather_Condition\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeather_Condition\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(mapper)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeather_Condition\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum)\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\core\\generic.py:6452\u001b[0m, in \u001b[0;36mNDFrame.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   6342\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   6343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m: NDFrameT, deep: bool_t \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m   6344\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6345\u001b[0m \u001b[38;5;124;03m    Make a copy of this object's indices and data.\u001b[39;00m\n\u001b[0;32m   6346\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6450\u001b[0m \u001b[38;5;124;03m    dtype: object\u001b[39;00m\n\u001b[0;32m   6451\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 6452\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n\u001b[0;32m   6454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\core\\internals\\managers.py:664\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    661\u001b[0m         res\u001b[38;5;241m.\u001b[39m_blklocs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blklocs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 664\u001b[0m     \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1829\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1823\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1824\u001b[0m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[0;32m   1825\u001b[0m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[0;32m   1826\u001b[0m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[0;32m   1827\u001b[0m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[1;32m-> 1829\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1830\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1831\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2272\u001b[0m, in \u001b[0;36m_consolidate\u001b[1;34m(blocks)\u001b[0m\n\u001b[0;32m   2270\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2271\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[1;32m-> 2272\u001b[0m     merged_blocks, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2273\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[0;32m   2274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2275\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[0;32m   2276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2304\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[1;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[0;32m   2301\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m bvals2[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_concat_same_type(bvals2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   2303\u001b[0m argsort \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(new_mgr_locs)\n\u001b[1;32m-> 2304\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mnew_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43margsort\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2305\u001b[0m new_mgr_locs \u001b[38;5;241m=\u001b[39m new_mgr_locs[argsort]\n\u001b[0;32m   2307\u001b[0m bp \u001b[38;5;241m=\u001b[39m BlockPlacement(new_mgr_locs)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 767. MiB for an array with shape (13, 7728394) and data type object"
     ]
    }
   ],
   "source": [
    "df_test = df_refined.copy()\n",
    "df_test['Weather_Condition'] = df_test['Weather_Condition'].map(mapper)\n",
    "print(df_test['Weather_Condition'].isna().sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7728389    NaN\n",
       "7728390    NaN\n",
       "7728391    NaN\n",
       "7728392    NaN\n",
       "7728393    NaN\n",
       "Name: Weather_Condition, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['Weather_Condition'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Business Insight/Recommendation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Insight/Recommendation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Insight/Recommendation 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tableau Dashboard link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "Text here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
