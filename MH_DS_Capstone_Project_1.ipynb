{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Add a relevant banner image here](path_to_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Short project description. Your bottom line up front (BLUF) insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "Text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load relevant imports here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import csv\n",
    "from datetime import time\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Severity     Start_Lat     Start_Lng       End_Lat       End_Lng  \\\n",
      "count  7.728394e+06  7.728394e+06  7.728394e+06  4.325632e+06  4.325632e+06   \n",
      "mean   2.212384e+00  3.620119e+01 -9.470255e+01  3.626183e+01 -9.572557e+01   \n",
      "std    4.875313e-01  5.076079e+00  1.739176e+01  5.272905e+00  1.810793e+01   \n",
      "min    1.000000e+00  2.455480e+01 -1.246238e+02  2.456601e+01 -1.245457e+02   \n",
      "25%    2.000000e+00  3.339963e+01 -1.172194e+02  3.346207e+01 -1.177543e+02   \n",
      "50%    2.000000e+00  3.582397e+01 -8.776662e+01  3.618349e+01 -8.802789e+01   \n",
      "75%    2.000000e+00  4.008496e+01 -8.035368e+01  4.017892e+01 -8.024709e+01   \n",
      "max    4.000000e+00  4.900220e+01 -6.711317e+01  4.907500e+01 -6.710924e+01   \n",
      "\n",
      "       Distance(mi)  Temperature(F)  Wind_Chill(F)   Humidity(%)  \\\n",
      "count  7.728394e+06    7.564541e+06   5.729375e+06  7.554250e+06   \n",
      "mean   5.618423e-01    6.166329e+01   5.825105e+01  6.483104e+01   \n",
      "std    1.776811e+00    1.901365e+01   2.238983e+01  2.282097e+01   \n",
      "min    0.000000e+00   -8.900000e+01  -8.900000e+01  1.000000e+00   \n",
      "25%    0.000000e+00    4.900000e+01   4.300000e+01  4.800000e+01   \n",
      "50%    3.000000e-02    6.400000e+01   6.200000e+01  6.700000e+01   \n",
      "75%    4.640000e-01    7.600000e+01   7.500000e+01  8.400000e+01   \n",
      "max    4.417500e+02    2.070000e+02   2.070000e+02  1.000000e+02   \n",
      "\n",
      "       Pressure(in)  Visibility(mi)  Wind_Speed(mph)  Precipitation(in)  \n",
      "count  7.587715e+06    7.551296e+06     7.157161e+06       5.524808e+06  \n",
      "mean   2.953899e+01    9.090376e+00     7.685490e+00       8.407210e-03  \n",
      "std    1.006190e+00    2.688316e+00     5.424983e+00       1.102246e-01  \n",
      "min    0.000000e+00    0.000000e+00     0.000000e+00       0.000000e+00  \n",
      "25%    2.937000e+01    1.000000e+01     4.600000e+00       0.000000e+00  \n",
      "50%    2.986000e+01    1.000000e+01     7.000000e+00       0.000000e+00  \n",
      "75%    3.003000e+01    1.000000e+01     1.040000e+01       0.000000e+00  \n",
      "max    5.863000e+01    1.400000e+02     1.087000e+03       3.647000e+01  \n",
      "    ID   Source  Severity           Start_Time             End_Time  \\\n",
      "0  A-1  Source2         3  2016-02-08 05:46:00  2016-02-08 11:00:00   \n",
      "1  A-2  Source2         2  2016-02-08 06:07:59  2016-02-08 06:37:59   \n",
      "2  A-3  Source2         2  2016-02-08 06:49:27  2016-02-08 07:19:27   \n",
      "3  A-4  Source2         3  2016-02-08 07:23:34  2016-02-08 07:53:34   \n",
      "4  A-5  Source2         2  2016-02-08 07:39:07  2016-02-08 08:09:07   \n",
      "\n",
      "   Start_Lat  Start_Lng  End_Lat  End_Lng  Distance(mi)  ... Roundabout  \\\n",
      "0  39.865147 -84.058723      NaN      NaN          0.01  ...      False   \n",
      "1  39.928059 -82.831184      NaN      NaN          0.01  ...      False   \n",
      "2  39.063148 -84.032608      NaN      NaN          0.01  ...      False   \n",
      "3  39.747753 -84.205582      NaN      NaN          0.01  ...      False   \n",
      "4  39.627781 -84.188354      NaN      NaN          0.01  ...      False   \n",
      "\n",
      "  Station   Stop Traffic_Calming Traffic_Signal Turning_Loop Sunrise_Sunset  \\\n",
      "0   False  False           False          False        False          Night   \n",
      "1   False  False           False          False        False          Night   \n",
      "2   False  False           False           True        False          Night   \n",
      "3   False  False           False          False        False          Night   \n",
      "4   False  False           False           True        False            Day   \n",
      "\n",
      "  Civil_Twilight Nautical_Twilight Astronomical_Twilight  \n",
      "0          Night             Night                 Night  \n",
      "1          Night             Night                   Day  \n",
      "2          Night               Day                   Day  \n",
      "3            Day               Day                   Day  \n",
      "4            Day               Day                   Day  \n",
      "\n",
      "[5 rows x 46 columns]\n",
      "Index(['ID', 'Source', 'Severity', 'Start_Time', 'End_Time', 'Start_Lat',\n",
      "       'Start_Lng', 'End_Lat', 'End_Lng', 'Distance(mi)', 'Description',\n",
      "       'Street', 'City', 'County', 'State', 'Zipcode', 'Country', 'Timezone',\n",
      "       'Airport_Code', 'Weather_Timestamp', 'Temperature(F)', 'Wind_Chill(F)',\n",
      "       'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Direction',\n",
      "       'Wind_Speed(mph)', 'Precipitation(in)', 'Weather_Condition', 'Amenity',\n",
      "       'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway',\n",
      "       'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal',\n",
      "       'Turning_Loop', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight',\n",
      "       'Astronomical_Twilight'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_all_data = pd.read_csv('Data/US_Accidents_March23.csv')\n",
    "print(df_all_data.describe())\n",
    "print(df_all_data.head())\n",
    "print(df_all_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_severity_by_state = pd.crosstab(df_all_data['Severity'], df_all_data['State'])\n",
    "df_severity_by_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart of accident severity by state\n",
    "states = df_severity_by_state.columns\n",
    "sev1 = df_severity_by_state.iloc[0]\n",
    "sev2 = df_severity_by_state.iloc[1]\n",
    "sev3 = df_severity_by_state.iloc[2]\n",
    "sev4 = ct_sev4_by_state = df_severity_by_state.iloc[3]\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.bar(states, sev1)\n",
    "plt.bar(states, sev2, bottom=sev1)\n",
    "plt.bar(states, sev3, bottom=sev1+sev2)\n",
    "plt.bar(states, sev4, bottom=sev1+sev2+sev3)\n",
    "plt.xlabel(\"State\")\n",
    "plt.xticks(rotation=60)\n",
    "plt.ylabel(\"Number of Accidents\")\n",
    "plt.legend([\"Severity 1\", \"Severity 2\", \"Severity 3\", \"Severity 4\"])\n",
    "plt.title(\"Accident Severity by State (2016 - 2023)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart of accident severity by state without sev2 (sev 2 >> than the others so obscures sev 1, 3, 4 above)\n",
    "states = df_severity_by_state.columns\n",
    "sev1 = df_severity_by_state.iloc[0]\n",
    "sev3 = df_severity_by_state.iloc[2]\n",
    "sev4 = ct_sev4_by_state = df_severity_by_state.iloc[3]\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.bar(states, sev1)\n",
    "plt.bar(states, sev3)\n",
    "plt.bar(states, sev4)\n",
    "plt.xlabel(\"State\")\n",
    "plt.xticks(rotation=60)\n",
    "plt.ylabel(\"Number of Accidents\")\n",
    "plt.legend([\"Severity 1\", \"Severity 3\", \"Severity 4\"])\n",
    "plt.title(\"Accident Severity by State, Excluding Severity 2 (2016 - 2023)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sev_by_crossing = pd.crosstab(df_all_data['Severity'], df_all_data['Crossing'])\n",
    "df_sev_by_crossing = df_sev_by_crossing.rename(columns={False: \"No\", True: \"Yes\"})\n",
    "df_sev_by_crossing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# charts of accident severity by bump, traffic calming, roundabout\n",
    "# chart of accident severity by state\n",
    "crossing = df_sev_by_crossing.columns\n",
    "cr1 = df_sev_by_crossing.iloc[0]\n",
    "cr2 = df_sev_by_crossing.iloc[1]\n",
    "cr3 = df_sev_by_crossing.iloc[2]\n",
    "cr4 = df_sev_by_crossing.iloc[3]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.bar(crossing, cr1)\n",
    "plt.bar(crossing, cr2)\n",
    "plt.bar(crossing, cr3)\n",
    "plt.bar(crossing, cr4)\n",
    "plt.xticks(rotation=60)\n",
    "plt.xlabel(\"Nearby Crossing\")\n",
    "plt.ylabel(\"Number of Accidents\")\n",
    "plt.legend([\"Severity 1\", \"Severity 2\", \"Severity 3\", \"Severity 4\"])\n",
    "plt.title(\"Accident Severity by Proximity to Crossing, (2016 - 2023)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Count of Wind Direction Entries: {df_all_data['Wind_Direction'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Selection\n",
    "\n",
    "Based on my exploration of the data, I'm dropping the following fields from the dataset for the following reasons:\n",
    "\n",
    "- Source: contains information that has no relationship to causes and effects of accidents\n",
    "- Start_Lat, Start_Lng, End_Lat, End_Lng, Street, City, County, State, Zipcode, Country: I'm going to model factors that influence accident severity, independent of the part of the country the accident occurs in.\n",
    "    - I'll retain Zipcode and State until after data cleaning and construction since they'll be used for imputation and creating a new field\n",
    "- Description: unstructured text that will not give meaningful results with the planned modeling\n",
    "- Timezone: duplicates zip/state with less precision\n",
    "- Street: this field is not standardized and will introduce noise to the modeling\n",
    "- Country: all data is from the United States so this field is redundant\n",
    "- Airport_Code: doesn't provide germane information-the exact location where weather conditions are reported is not a variable that can be adjusted\n",
    "- Weather_Timestamp: not related to the conditions of the accidents in any way\n",
    "- Wind_Direction: too many unique values; values are also not related to travel directions so it's unlikely they'll  produce clear/actionable conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Severity  Distance(mi)  Temperature(F)  Wind_Chill(F)  \\\n",
      "count  7.728394e+06  7.728394e+06    7.564541e+06   5.729375e+06   \n",
      "mean   2.212384e+00  5.618423e-01    6.166329e+01   5.825105e+01   \n",
      "std    4.875313e-01  1.776811e+00    1.901365e+01   2.238983e+01   \n",
      "min    1.000000e+00  0.000000e+00   -8.900000e+01  -8.900000e+01   \n",
      "25%    2.000000e+00  0.000000e+00    4.900000e+01   4.300000e+01   \n",
      "50%    2.000000e+00  3.000000e-02    6.400000e+01   6.200000e+01   \n",
      "75%    2.000000e+00  4.640000e-01    7.600000e+01   7.500000e+01   \n",
      "max    4.000000e+00  4.417500e+02    2.070000e+02   2.070000e+02   \n",
      "\n",
      "        Humidity(%)  Pressure(in)  Visibility(mi)  Wind_Speed(mph)  \\\n",
      "count  7.554250e+06  7.587715e+06    7.551296e+06     7.157161e+06   \n",
      "mean   6.483104e+01  2.953899e+01    9.090376e+00     7.685490e+00   \n",
      "std    2.282097e+01  1.006190e+00    2.688316e+00     5.424983e+00   \n",
      "min    1.000000e+00  0.000000e+00    0.000000e+00     0.000000e+00   \n",
      "25%    4.800000e+01  2.937000e+01    1.000000e+01     4.600000e+00   \n",
      "50%    6.700000e+01  2.986000e+01    1.000000e+01     7.000000e+00   \n",
      "75%    8.400000e+01  3.003000e+01    1.000000e+01     1.040000e+01   \n",
      "max    1.000000e+02  5.863000e+01    1.400000e+02     1.087000e+03   \n",
      "\n",
      "       Precipitation(in)  \n",
      "count       5.524808e+06  \n",
      "mean        8.407210e-03  \n",
      "std         1.102246e-01  \n",
      "min         0.000000e+00  \n",
      "25%         0.000000e+00  \n",
      "50%         0.000000e+00  \n",
      "75%         0.000000e+00  \n",
      "max         3.647000e+01  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7728394 entries, 0 to 7728393\n",
      "Data columns (total 31 columns):\n",
      " #   Column                 Dtype  \n",
      "---  ------                 -----  \n",
      " 0   Severity               int64  \n",
      " 1   Start_Time             object \n",
      " 2   End_Time               object \n",
      " 3   Distance(mi)           float64\n",
      " 4   State                  object \n",
      " 5   Zipcode                object \n",
      " 6   Temperature(F)         float64\n",
      " 7   Wind_Chill(F)          float64\n",
      " 8   Humidity(%)            float64\n",
      " 9   Pressure(in)           float64\n",
      " 10  Visibility(mi)         float64\n",
      " 11  Wind_Speed(mph)        float64\n",
      " 12  Precipitation(in)      float64\n",
      " 13  Weather_Condition      object \n",
      " 14  Amenity                bool   \n",
      " 15  Bump                   bool   \n",
      " 16  Crossing               bool   \n",
      " 17  Give_Way               bool   \n",
      " 18  Junction               bool   \n",
      " 19  No_Exit                bool   \n",
      " 20  Railway                bool   \n",
      " 21  Roundabout             bool   \n",
      " 22  Station                bool   \n",
      " 23  Stop                   bool   \n",
      " 24  Traffic_Calming        bool   \n",
      " 25  Traffic_Signal         bool   \n",
      " 26  Turning_Loop           bool   \n",
      " 27  Sunrise_Sunset         object \n",
      " 28  Civil_Twilight         object \n",
      " 29  Nautical_Twilight      object \n",
      " 30  Astronomical_Twilight  object \n",
      "dtypes: bool(13), float64(8), int64(1), object(9)\n",
      "memory usage: 1.1+ GB\n",
      "None\n",
      "Index(['Severity', 'Start_Time', 'End_Time', 'Distance(mi)', 'State',\n",
      "       'Zipcode', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)',\n",
      "       'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)',\n",
      "       'Precipitation(in)', 'Weather_Condition', 'Amenity', 'Bump', 'Crossing',\n",
      "       'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station',\n",
      "       'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop',\n",
      "       'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight',\n",
      "       'Astronomical_Twilight'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df_refined = df_all_data.drop(['ID', 'Source', 'Start_Lat', 'Start_Lng', 'End_Lat', 'End_Lng', 'Description',\n",
    "                                'Timezone', 'Street', 'City', 'County', 'Country', 'Airport_Code', 'Weather_Timestamp', 'Wind_Direction'], axis=1)\n",
    "\n",
    "print(df_refined.describe())\n",
    "print(df_refined.info())\n",
    "print(df_refined.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_all_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Missing Values\n",
    "\n",
    "I've managed missing values in the code blocks below. Here's a brief explanation of my approch to each column:\n",
    "\n",
    "- Temperature, Wind_Chill, Humidity, Pressure, Visibility: imputed based on the mean temp of other accident entries sharing the same day and zip code (or state if there are none in the zip code)\n",
    "- Precipitation, Wind_Speed: assumed NaN indicates no precipitation/wind and replaced NaN with zero\n",
    "- Sunrise_Sunset: imputed based on\n",
    "- Civil_Twilight: imputed based on\n",
    "- Nautical_Twilight: imputed based on\n",
    "- Astronomical_Twilight: imputed based on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refined['Start_Time'] = pd.to_datetime(df_refined['Start_Time'], yearfirst=True, format='mixed')\n",
    "df_refined['End_Time'] = pd.to_datetime(df_refined['End_Time'], yearfirst=True, format='mixed')\n",
    "df_refined['Acc_date'] = df_refined['Start_Time'].dt.date\n",
    "df_refined['Acc_time'] = df_refined['Start_Time'].dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: 158386 temperature entries still missing after date-zip fill.\n",
      "Filling remaining with date-state mean.\n",
      "Warning: 7341 temperature entries still missing after date-state fill.\n",
      "Filling remaining with overall mean as last resort.\n"
     ]
    }
   ],
   "source": [
    "# replacing NaN values in the temperature column\n",
    "\n",
    "def fill_missing_temp(df, date='Acc_date', zip='Zipcode', state='State', temp='Temperature(F)'):\n",
    "    \"\"\"\n",
    "    Replaces NaN humidity values with the mean humidity of entries\n",
    "    with the same date and zip code. If no match exists with date and county,\n",
    "    uses date and state.\n",
    "    \"\"\"\n",
    "    # Step 1: mean humidity for each date-zip combination\n",
    "    df_temp_fill = df_refined.copy()\n",
    "    \n",
    "    temp_means_zip = df_temp_fill.groupby([date, zip])[temp].transform('mean')\n",
    "    \n",
    "    # fill NaN values with the date-zip group mean\n",
    "    df_temp_fill[temp] = df_temp_fill[temp].fillna(temp_means_zip)\n",
    "    \n",
    "    # Step 2: date-state combination for remaining NaNs\n",
    "    if df_temp_fill[temp].isna().any():\n",
    "        remaining = df_temp_fill[temp].isna().sum()\n",
    "        print(f\"Info: {remaining} temperature entries still missing after date-zip fill.\")\n",
    "        print(\"Filling remaining with date-state mean.\")\n",
    "        \n",
    "        temp_means_state = df_temp_fill.groupby([date, state])[temp].transform('mean')\n",
    "        df_temp_fill[temp] = df_temp_fill[temp].fillna(temp_means_state)\n",
    "    \n",
    "    # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "    if df_temp_fill[temp].isna().any():\n",
    "        remaining = df_temp_fill[temp].isna().sum()\n",
    "        print(f\"Warning: {remaining} temperature entries still missing after date-state fill.\")\n",
    "        print(\"Filling remaining with overall mean as last resort.\")\n",
    "        df_temp_fill[temp] = df_temp_fill[temp].fillna(df_temp_fill[temp].mean())\n",
    "    \n",
    "    return df_temp_fill\n",
    "\n",
    "df_temp_fill = fill_missing_temp(df_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying NaNs have been replaced\n",
    "df_temp_fill['Temperature(F)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with NaN Temperature: 0\n"
     ]
    }
   ],
   "source": [
    "# writing NaN-free column back to the original dataframe\n",
    "df_refined['Temperature(F)'] = df_temp_fill['Temperature(F)']\n",
    "print(f\"Entries with NaN Temperature: {df_refined['Temperature(F)'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deleting working df to free up memory\n",
    "del df_temp_fill\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: 1949795 wind chills still missing after date-zip fill.\n",
      "Filling remaining with date-state mean.\n",
      "Warning: 687931 wind chills still missing after date-state fill.\n",
      "Filling remaining with overall mean as last resort.\n"
     ]
    }
   ],
   "source": [
    "# replacing NaN values in the wind chill column\n",
    "\n",
    "def fill_missing_windchill(df, date='Acc_date', zip='Zipcode', state='State', windchill='Wind_Chill(F)'):\n",
    "    \"\"\"\n",
    "    Replaces NaN wind chill values with the mean wind chill of entries\n",
    "    with the same date and zip code. If no match exists with date and county,\n",
    "    uses date and state.\n",
    "    \"\"\"\n",
    "    # Step 1: mean temperature for each date-zip combination\n",
    "    df_windchill_fill = df_refined.copy()\n",
    "    \n",
    "    windchill_means_zip = df_windchill_fill.groupby([date, zip])[windchill].transform('mean')\n",
    "    \n",
    "    # fill NaN values with the date-zip group mean\n",
    "    df_windchill_fill[windchill] = df_windchill_fill[windchill].fillna(windchill_means_zip)\n",
    "    \n",
    "    # Step 2: date-state combination for remaining NaNs\n",
    "    if df_windchill_fill[windchill].isna().any():\n",
    "        remaining = df_windchill_fill[windchill].isna().sum()\n",
    "        print(f\"Info: {remaining} wind chills still missing after date-zip fill.\")\n",
    "        print(\"Filling remaining with date-state mean.\")\n",
    "        \n",
    "        windchill_means_state = df_windchill_fill.groupby([date, state])[windchill].transform('mean')\n",
    "        df_windchill_fill[windchill] = df_windchill_fill[windchill].fillna(windchill_means_state)\n",
    "    \n",
    "    # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "    if df_windchill_fill[windchill].isna().any():\n",
    "        remaining = df_windchill_fill[windchill].isna().sum()\n",
    "        print(f\"Warning: {remaining} wind chills still missing after date-state fill.\")\n",
    "        print(\"Filling remaining with overall mean as last resort.\")\n",
    "        df_windchill_fill[windchill] = df_windchill_fill[windchill].fillna(df_windchill_fill[windchill].mean())\n",
    "    \n",
    "    return df_windchill_fill\n",
    "\n",
    "df_windchill_fill = fill_missing_windchill(df_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying NaNs have been replaced in the working df\n",
    "df_windchill_fill['Wind_Chill(F)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with NaN Wind Chill: 0\n"
     ]
    }
   ],
   "source": [
    "# writing NaN-free column back to the original dataframe\n",
    "df_refined['Wind_Chill(F)'] = df_windchill_fill['Wind_Chill(F)']\n",
    "print(f\"Entries with NaN Wind Chill: {df_refined['Wind_Chill(F)'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deleting working df to free up memory\n",
    "del df_windchill_fill\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: 166329 humidity entries still missing after date-zip fill.\n",
      "Filling remaining with date-state mean.\n",
      "Warning: 7358 humidity entries still missing after date-state fill.\n",
      "Filling remaining with overall mean as last resort.\n"
     ]
    }
   ],
   "source": [
    "# replacing NaN values in the humidity column\n",
    "\n",
    "def fill_missing_hum(df, date='Acc_date', zip='Zipcode', state='State', hum='Humidity(%)'):\n",
    "    \"\"\"\n",
    "    Replaces NaN humidity values with the mean humidity of entries\n",
    "    with the same date and zip code. If no match exists with date and county,\n",
    "    uses date and state.\n",
    "    \"\"\"\n",
    "    # Step 1: mean humidity for each date-zip combination\n",
    "    df_hum_fill = df_refined.copy()\n",
    "    \n",
    "    hum_means_zip = df_hum_fill.groupby([date, zip])[hum].transform('mean')\n",
    "    \n",
    "    # fill NaN values with the date-zip group mean\n",
    "    df_hum_fill[hum] = df_hum_fill[hum].fillna(hum_means_zip)\n",
    "    \n",
    "    # Step 2: date-state combination for remaining NaNs\n",
    "    if df_hum_fill[hum].isna().any():\n",
    "        remaining = df_hum_fill[hum].isna().sum()\n",
    "        print(f\"Info: {remaining} humidity entries still missing after date-zip fill.\")\n",
    "        print(\"Filling remaining with date-state mean.\")\n",
    "        \n",
    "        hum_means_state = df_hum_fill.groupby([date, state])[hum].transform('mean')\n",
    "        df_hum_fill[hum] = df_hum_fill[hum].fillna(hum_means_state)\n",
    "    \n",
    "    # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "    if df_hum_fill[hum].isna().any():\n",
    "        remaining = df_hum_fill[hum].isna().sum()\n",
    "        print(f\"Warning: {remaining} humidity entries still missing after date-state fill.\")\n",
    "        print(\"Filling remaining with overall mean as last resort.\")\n",
    "        df_hum_fill[hum] = df_hum_fill[hum].fillna(df_hum_fill[hum].mean())\n",
    "    \n",
    "    return df_hum_fill\n",
    "\n",
    "df_hum_fill = fill_missing_hum(df_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying NaNs have been replaced in the working df\n",
    "df_hum_fill['Humidity(%)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with NaN humidity: 0\n"
     ]
    }
   ],
   "source": [
    "# writing NaN-free column back to the original dataframe\n",
    "df_refined['Humidity(%)'] = df_hum_fill['Humidity(%)']\n",
    "print(f\"Entries with NaN humidity: {df_refined['Humidity(%)'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deleting working df to free up memory\n",
    "del df_hum_fill\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: 137561 pressure entries still missing after date-zip fill.\n",
      "Filling remaining with date-state mean.\n",
      "Warning: 7313 pressure entries still missing after date-state fill.\n",
      "Filling remaining with overall mean as last resort.\n"
     ]
    }
   ],
   "source": [
    "# replacing NaN values in the pressure column\n",
    "\n",
    "def fill_missing_press(df, date='Acc_date', zip='Zipcode', state='State', press='Pressure(in)'):\n",
    "    \"\"\"\n",
    "    Replaces NaN pressure values with the mean pressure of entries\n",
    "    with the same date and zip code. If no match exists with date and county,\n",
    "    uses date and state.\n",
    "    \"\"\"\n",
    "    # Step 1: mean pressure for each date-zip combination\n",
    "    df_press_fill = df_refined.copy()\n",
    "    \n",
    "    press_means_zip = df_press_fill.groupby([date, zip])[press].transform('mean')\n",
    "    \n",
    "    # fill NaN values with the date-zip group mean\n",
    "    df_press_fill[press] = df_press_fill[press].fillna(press_means_zip)\n",
    "    \n",
    "    # Step 2: date-state combination for remaining NaNs\n",
    "    if df_press_fill[press].isna().any():\n",
    "        remaining = df_press_fill[press].isna().sum()\n",
    "        print(f\"Info: {remaining} pressure entries still missing after date-zip fill.\")\n",
    "        print(\"Filling remaining with date-state mean.\")\n",
    "        \n",
    "        press_means_state = df_press_fill.groupby([date, state])[press].transform('mean')\n",
    "        df_press_fill[press] = df_press_fill[press].fillna(press_means_state)\n",
    "    \n",
    "    # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "    if df_press_fill[press].isna().any():\n",
    "        remaining = df_press_fill[press].isna().sum()\n",
    "        print(f\"Warning: {remaining} pressure entries still missing after date-state fill.\")\n",
    "        print(\"Filling remaining with overall mean as last resort.\")\n",
    "        df_press_fill[press] = df_press_fill[press].fillna(df_press_fill[press].mean())\n",
    "    \n",
    "    return df_press_fill\n",
    "\n",
    "df_press_fill = fill_missing_press(df_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying NaNs have been replaced in the working df\n",
    "df_press_fill['Pressure(in)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with NaN pressure: 0\n"
     ]
    }
   ],
   "source": [
    "# writing NaN-free column back to the original dataframe\n",
    "df_refined['Pressure(in)'] = df_press_fill['Pressure(in)']\n",
    "print(f\"Entries with NaN pressure: {df_refined['Pressure(in)'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deleting working df to free up memory\n",
    "del df_press_fill\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: 171980 visibility entries still missing after date-zip fill.\n",
      "Filling remaining with date-state mean.\n",
      "Warning: 7409 visibility entries still missing after date-state fill.\n",
      "Filling remaining with overall mean as last resort.\n"
     ]
    }
   ],
   "source": [
    "# replacing NaN values in the visibility column\n",
    "\n",
    "def fill_missing_vis(df, date='Acc_date', zip='Zipcode', state='State', vis='Visibility(mi)'):\n",
    "    \"\"\"\n",
    "    Replaces NaN visibility values with the mean visibility of entries\n",
    "    with the same date and zip code. If no match exists with date and county,\n",
    "    uses date and state.\n",
    "    \"\"\"\n",
    "    # Step 1: mean visibility for each date-zip combination\n",
    "    df_vis_fill = df_refined.copy()\n",
    "    \n",
    "    vis_means_zip = df_vis_fill.groupby([date, zip])[vis].transform('mean')\n",
    "    \n",
    "    # fill NaN values with the date-zip group mean\n",
    "    df_vis_fill[vis] = df_vis_fill[vis].fillna(vis_means_zip)\n",
    "    \n",
    "    # Step 2: date-state combination for remaining NaNs\n",
    "    if df_vis_fill[vis].isna().any():\n",
    "        remaining = df_vis_fill[vis].isna().sum()\n",
    "        print(f\"Info: {remaining} visibility entries still missing after date-zip fill.\")\n",
    "        print(\"Filling remaining with date-state mean.\")\n",
    "        \n",
    "        vis_means_state = df_vis_fill.groupby([date, state])[vis].transform('mean')\n",
    "        df_vis_fill[vis] = df_vis_fill[vis].fillna(vis_means_state)\n",
    "    \n",
    "    # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "    if df_vis_fill[vis].isna().any():\n",
    "        remaining = df_vis_fill[vis].isna().sum()\n",
    "        print(f\"Warning: {remaining} visibility entries still missing after date-state fill.\")\n",
    "        print(\"Filling remaining with overall mean as last resort.\")\n",
    "        df_vis_fill[vis] = df_vis_fill[vis].fillna(df_vis_fill[vis].mean())\n",
    "    \n",
    "    return df_vis_fill\n",
    "\n",
    "df_vis_fill = fill_missing_vis(df_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying NaNs have been replaced in the working df\n",
    "df_vis_fill['Visibility(mi)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with NaN visibility: 0\n"
     ]
    }
   ],
   "source": [
    "# writing NaN-free column back to the original dataframe\n",
    "df_refined['Visibility(mi)'] = df_vis_fill['Visibility(mi)']\n",
    "print(f\"Entries with NaN visibility: {df_refined['Visibility(mi)'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deleting working df to free up memory\n",
    "del df_vis_fill\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries with NaN Wind Speed: 0\n",
      "Entries with NaN Precipitation: 0\n"
     ]
    }
   ],
   "source": [
    "# replacing NaN with 0 in wind speed and precipitation\n",
    "\n",
    "df_refined['Wind_Speed(mph)'] = df_refined['Wind_Speed(mph)'].fillna(0)\n",
    "df_refined['Precipitation(in)'] = df_refined['Precipitation(in)'].fillna(0)\n",
    "print(f\"Entries with NaN Wind Speed: {df_refined['Wind_Speed(mph)'].isna().sum()}\")\n",
    "print(f\"Entries with NaN Precipitation: {df_refined['Precipitation(in)'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Severity', 'Start_Time', 'End_Time', 'Distance(mi)', 'State',\n",
       "       'Zipcode', 'Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)',\n",
       "       'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)',\n",
       "       'Precipitation(in)', 'Weather_Condition', 'Amenity', 'Bump', 'Crossing',\n",
       "       'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station',\n",
       "       'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop',\n",
       "       'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight',\n",
       "       'Astronomical_Twilight', 'Acc_date', 'Acc_time'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_refined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting boolean to integer\n",
    "df_refined['Amenity'] = df_refined['Amenity'].astype(int)\n",
    "df_refined['Bump'] = df_refined['Bump'].astype(int)\n",
    "df_refined['Crossing'] = df_refined['Crossing'].astype(int)\n",
    "df_refined['Give_Way'] = df_refined['Give_Way'].astype(int)\n",
    "df_refined['Junction'] = df_refined['Junction'].astype(int)\n",
    "df_refined['No_Exit'] = df_refined['No_Exit'].astype(int)\n",
    "df_refined['Railway'] = df_refined['Railway'].astype(int)\n",
    "df_refined['Roundabout'] = df_refined['Roundabout'].astype(int)\n",
    "df_refined['Station'] = df_refined['Station'].astype(int)\n",
    "df_refined['Stop'] = df_refined['Stop'].astype(int)\n",
    "df_refined['Traffic_Calming'] = df_refined['Traffic_Calming'].astype(int)\n",
    "df_refined['Traffic_Signal'] = df_refined['Traffic_Signal'].astype(int)\n",
    "df_refined['Turning_Loop'] = df_refined['Turning_Loop'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7705148\n",
       "4      23246\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking to see if the day/night data points are missing for the same entries\n",
    "day_night_cols =['Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']\n",
    "blank_counts = df_refined[day_night_cols].isna().sum(axis=1)\n",
    "results = blank_counts.value_counts().sort_index()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del blank_counts\n",
    "del results\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Day_Night_Calc\n",
       "Day      5515796\n",
       "Night    2212598\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating column displaying Day or Night based on a day with equal length days and nights\n",
    "def day_or_night(t):\n",
    "    if time(6, 0, 0) <= t < time(18, 0, 0):\n",
    "        return 'Day'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "df_refined['Day_Night_Calc'] = df_refined['Acc_time'].apply(day_or_night)\n",
    "df_refined['Day_Night_Calc'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refined_dup2 = df_refined.copy()\n",
    "\n",
    "# replacing missing day/night values with the generic day/night column created above\n",
    "for col in day_night_cols:\n",
    "    df_refined_dup2[col] = np.where(df_refined_dup2[col].isna(), df_refined_dup2['Day_Night_Calc'], df_refined_dup2[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7728394\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying that no empty entries remain\n",
    "blank_counts2 = df_refined_dup2[day_night_cols].isna().sum(axis=1)\n",
    "results2 = blank_counts2.value_counts().sort_index()\n",
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refined['Sunrise_Sunset'] = df_refined_dup2['Sunrise_Sunset']\n",
    "df_refined['Civil_Twilight'] = df_refined_dup2['Civil_Twilight']\n",
    "df_refined['Nautical_Twilight'] = df_refined_dup2['Nautical_Twilight']\n",
    "df_refined['Astronomical_Twilight'] = df_refined_dup2['Astronomical_Twilight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_refined_dup2\n",
    "del results2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Weather Condition\n",
    "\n",
    "The Weather_Condition field has a large number of values across the dataset. Many of these values are used rarely and will complicate modeling, so I'll consolidate them into a smaller number of value options. This will be a manual process-I'll review and create a mapping table by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Count of Unique Weather Condition Entries: {df_all_data['Weather_Condition'].nunique()}\")\n",
    "df_refined['Weather_Condition'].value_counts().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing a list of weather condition unique values to csv\n",
    "unique_wthr_cond = pd.Series(df_refined['Weather_Condition'].unique())\n",
    "unique_wthr_cond.to_csv('conditions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_dict_reader(filename):\n",
    "    result = {}\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if len(row) >=2:\n",
    "                result[row[0]] = row[1]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather_Condition\n",
      "Hail                             133\n",
      "Freezing Rain                    262\n",
      "Sleet                            319\n",
      "Blowing Ash / Dust / Sand        769\n",
      "Light Freezing Fog              1001\n",
      "Blowing Snow                    1673\n",
      "Mist                            3554\n",
      "Light Sleet                     5319\n",
      "Heavy Snow                      5922\n",
      "Light Fog                       7271\n",
      "Smoke                          12764\n",
      "Wintry  Mix                    12768\n",
      "Heavy Thunderstorm             13259\n",
      "Snow                           16850\n",
      "Tornado                        17664\n",
      "Thunderstorm                   21501\n",
      "Light Thunderstorm             22545\n",
      "Heavy Rain                     33843\n",
      "Hazy                           77828\n",
      "Rain                           87340\n",
      "Fog                            99981\n",
      "Light Snow                    135662\n",
      "Light Rain                    402997\n",
      "Partly Cloudy                 915556\n",
      "Cloudy                       2249686\n",
      "Fair                         3408468\n",
      "Name: count, dtype: int64\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "weather_mapper = csv_to_dict_reader('conditions_consolidated.csv')\n",
    "df_refined_dup1 = df_refined.copy()\n",
    "\n",
    "df_refined_dup1['Weather_Condition'] = df_refined_dup1['Weather_Condition'].map(weather_mapper)\n",
    "print(df_refined_dup1['Weather_Condition'].value_counts().sort_values(ascending=True))\n",
    "print(df_refined_dup1['Weather_Condition'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_refined['Weather_Condition'] = df_refined_dup1['Weather_Condition']\n",
    "df_refined['Weather_Condition'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_refined_dup1\n",
    "del weather_mapper\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing Data\n",
    "\n",
    "I'm adding a feature that categorizes the location of the accidents as urban / suburban / rural. I'm using Rural-Urban Commuting Area Codes from the USDA Economic Research Service as the data source. https://www.ers.usda.gov/data-products/rural-urban-commuting-area-codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_type_mapper = csv_to_dict_reader('category_by_zip.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1919"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting zip code entries to the 5-digit format\n",
    "\n",
    "df_refined['Zipcode'] = df_refined['Zipcode'].astype(str)\n",
    "df_refined['Zipcode'] = df_refined['Zipcode'].str[:5]\n",
    "\n",
    "# mapping a new location type field using the mapper created above\n",
    "df_refined['Location_Type'] = df_refined['Zipcode'].map(loc_type_mapper)\n",
    "df_refined['Location_Type'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7794       nan\n",
       "13889      nan\n",
       "14515      nan\n",
       "16208      nan\n",
       "23539      nan\n",
       "          ... \n",
       "7722065    nan\n",
       "7723723    nan\n",
       "7724048    nan\n",
       "7724049    nan\n",
       "7726723    nan\n",
       "Name: Zipcode, Length: 1919, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for clear pattern(s) in the missing location type rows\n",
    "nan_zip_codes = df_refined.loc[df_refined['Location_Type'].isna(), 'Zipcode']\n",
    "nan_zip_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Location_Type\n",
       "Urban       6426638\n",
       "Suburban     781803\n",
       "Rural        518034\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determining proportions of location types in the whole dataset; will impute the NaNs with the same proportion\n",
    "df_refined['Location_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling missing values in line with value distribution in the rest of the dataset\n",
    "location_types = ['Urban', 'Suburban', 'Rural']\n",
    "proportions = [0.83, 0.10, 0.07]\n",
    "\n",
    "num_missing = df_refined['Location_Type'].isna().sum()\n",
    "random_values = np.random.choice(location_types, size=num_missing, p=proportions)\n",
    "\n",
    "df_refined.loc[df_refined['Location_Type'].isna(), 'Location_Type'] = random_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifying all rows have been filled\n",
    "num_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Severity', 'Start_Time', 'End_Time', 'Distance(mi)', 'Temperature(F)',\n",
       "       'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)',\n",
       "       'Wind_Speed(mph)', 'Precipitation(in)', 'Weather_Condition', 'Amenity',\n",
       "       'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway',\n",
       "       'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal',\n",
       "       'Turning_Loop', 'Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight',\n",
       "       'Astronomical_Twilight', 'Location_Type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing fields created for cleaning\n",
    "df_cleaned = df_refined.drop(['State', 'Zipcode', 'Acc_date', 'Acc_time', 'Day_Night_Calc'], axis=1)\n",
    "df_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = pd.get_dummies(df_cleaned, columns=['Weather_Condition', 'Sunrise_Sunset', 'Civil_Twilight', \n",
    "                                                 'Nautical_Twilight', 'Astronomical_Twilight', 'Location_Type'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_cleaned\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Business Insight/Recommendation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Insight/Recommendation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Insight/Recommendation 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tableau Dashboard link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "Text here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
