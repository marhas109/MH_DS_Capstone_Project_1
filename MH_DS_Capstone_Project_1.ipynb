{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Add a relevant banner image here](path_to_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Short project description. Your bottom line up front (BLUF) insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "Text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting pyspark\n",
      "  Using cached pyspark-4.0.1.tar.gz (434.2 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting py4j==0.10.9.9 (from pyspark)\n",
      "  Using cached py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Using cached py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (pyproject.toml): started\n",
      "  Building wheel for pyspark (pyproject.toml): still running...\n",
      "  Building wheel for pyspark (pyproject.toml): still running...\n",
      "  Building wheel for pyspark (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-4.0.1-py2.py3-none-any.whl size=434813901 sha256=329e34e4aa9b27564dcb85fa215617a62ddd7dd63adec809aa20a51cddc68281\n",
      "  Stored in directory: c:\\users\\markh\\appdata\\local\\pip\\cache\\wheels\\10\\e6\\6b\\c50eb601fa827dd56a5272db5d5db360e559e527a80a665b1d\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "\n",
      "   ---------------------------------------- 0/2 [py4j]\n",
      "   ---------------------------------------- 0/2 [py4j]\n",
      "   ---------------------------------------- 0/2 [py4j]\n",
      "   ---------------------------------------- 0/2 [py4j]\n",
      "   ---------------------------------------- 0/2 [py4j]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   -------------------- ------------------- 1/2 [pyspark]\n",
      "   ---------------------------------------- 2/2 [pyspark]\n",
      "\n",
      "Successfully installed py4j-0.10.9.9 pyspark-4.0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load relevant imports here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import csv\n",
    "from datetime import time\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from scipy.stats import randint, reciprocal\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import min, max, col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapstone_proj_1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pyspark\\sql\\session.py:556\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    554\u001b[0m     sparkConf\u001b[38;5;241m.\u001b[39mset(key, value)\n\u001b[0;32m    555\u001b[0m \u001b[38;5;66;03m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msparkConf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pyspark\\core\\context.py:523\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 523\u001b[0m         \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pyspark\\core\\context.py:205\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m     )\n\u001b[1;32m--> 205\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    208\u001b[0m         master,\n\u001b[0;32m    209\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    220\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pyspark\\core\\context.py:444\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 444\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\site-packages\\pyspark\\java_gateway.py:104\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m     proc \u001b[38;5;241m=\u001b[39m Popen(command, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpopen_kwargs)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;66;03m# preexec_fn not supported on Windows\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     proc \u001b[38;5;241m=\u001b[39m Popen(command, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpopen_kwargs)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mpoll() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\subprocess.py:947\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    943\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[0;32m    944\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    945\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    948\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    949\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[1;32mc:\\Users\\markh\\.conda\\envs\\ai-environment\\lib\\subprocess.py:1416\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1414\u001b[0m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[0;32m   1415\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1416\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1417\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1418\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1419\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1420\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1421\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1422\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1423\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1424\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1425\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1426\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1431\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1432\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1433\u001b[0m                          errread, errwrite)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master('capstone_proj_1').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_all_data \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData/US_Accidents_March23.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_all_data\u001b[38;5;241m.\u001b[39mdescribe())\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_all_data\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "df_all_data = spark.read.csv('Data/US_Accidents_March23.csv')\n",
    "print(df_all_data.describe())\n",
    "print(df_all_data.head())\n",
    "print(df_all_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_severity_by_state = pd.crosstab(df_all_data['Severity'], df_all_data['State'])\n",
    "df_severity_by_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart of accident severity by state\n",
    "states = df_severity_by_state.columns\n",
    "sev1 = df_severity_by_state.iloc[0]\n",
    "sev2 = df_severity_by_state.iloc[1]\n",
    "sev3 = df_severity_by_state.iloc[2]\n",
    "sev4 = ct_sev4_by_state = df_severity_by_state.iloc[3]\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.bar(states, sev1)\n",
    "plt.bar(states, sev2, bottom=sev1)\n",
    "plt.bar(states, sev3, bottom=sev1+sev2)\n",
    "plt.bar(states, sev4, bottom=sev1+sev2+sev3)\n",
    "plt.xlabel(\"State\")\n",
    "plt.xticks(rotation=60)\n",
    "plt.ylabel(\"Number of Accidents\")\n",
    "plt.legend([\"Severity 1\", \"Severity 2\", \"Severity 3\", \"Severity 4\"])\n",
    "plt.title(\"Accident Severity by State (2016 - 2023)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart of accident severity by state without sev2 (sev 2 >> than the others so obscures sev 1, 3, 4 above)\n",
    "states = df_severity_by_state.columns\n",
    "sev1 = df_severity_by_state.iloc[0]\n",
    "sev3 = df_severity_by_state.iloc[2]\n",
    "sev4 = ct_sev4_by_state = df_severity_by_state.iloc[3]\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.bar(states, sev1)\n",
    "plt.bar(states, sev3)\n",
    "plt.bar(states, sev4)\n",
    "plt.xlabel(\"State\")\n",
    "plt.xticks(rotation=60)\n",
    "plt.ylabel(\"Number of Accidents\")\n",
    "plt.legend([\"Severity 1\", \"Severity 3\", \"Severity 4\"])\n",
    "plt.title(\"Accident Severity by State, Excluding Severity 2 (2016 - 2023)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sev_by_crossing = pd.crosstab(df_all_data['Severity'], df_all_data['Crossing'])\n",
    "df_sev_by_crossing = df_sev_by_crossing.rename(columns={False: \"No\", True: \"Yes\"})\n",
    "df_sev_by_crossing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# charts of accident severity by bump, traffic calming, roundabout\n",
    "# chart of accident severity by state\n",
    "crossing = df_sev_by_crossing.columns\n",
    "cr1 = df_sev_by_crossing.iloc[0]\n",
    "cr2 = df_sev_by_crossing.iloc[1]\n",
    "cr3 = df_sev_by_crossing.iloc[2]\n",
    "cr4 = df_sev_by_crossing.iloc[3]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.bar(crossing, cr1)\n",
    "plt.bar(crossing, cr2)\n",
    "plt.bar(crossing, cr3)\n",
    "plt.bar(crossing, cr4)\n",
    "plt.xticks(rotation=60)\n",
    "plt.xlabel(\"Nearby Crossing\")\n",
    "plt.ylabel(\"Number of Accidents\")\n",
    "plt.legend([\"Severity 1\", \"Severity 2\", \"Severity 3\", \"Severity 4\"])\n",
    "plt.title(\"Accident Severity by Proximity to Crossing, (2016 - 2023)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Count of Wind Direction Entries: {df_all_data['Wind_Direction'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Selection\n",
    "\n",
    "Based on my exploration of the data, I'm dropping the following fields from the dataset for the following reasons:\n",
    "\n",
    "- Source: contains information that has no relationship to causes and effects of accidents\n",
    "- Start_Lat, Start_Lng, End_Lat, End_Lng, Street, City, County, State, Zipcode, Country: I'm going to model factors that influence accident severity independent of location.\n",
    "    - I'll drop Zipcode and State data cleaning and construction since they'll be used for imputation and creating a new field\n",
    "- Start_Time, End_Time: as with location features, I'm dropping these because I'll model factors independent of time\n",
    "    - I'll drop Start_Time after imputation\n",
    "- Description: unstructured text that will not give meaningful results with the planned modeling\n",
    "- Timezone: duplicates zip/state with less precision\n",
    "- Street: this field is not standardized and will introduce noise to the modeling\n",
    "- Country: all data is from the United States so this field is redundant\n",
    "- Airport_Code: doesn't provide germane information-the exact location where weather conditions are reported is not a variable that can be adjusted\n",
    "- Weather_Timestamp: not related to the conditions of the accidents in any way\n",
    "- Wind_Direction: too many unique values; values are also not related to travel directions so it's unlikely they'll  produce clear/actionable conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refined = df_all_data.drop(['ID', 'Source', 'Start_Lat', 'Start_Lng', 'End_Lat', 'End_Lng', 'Description',\n",
    "                                'Timezone', 'Street', 'City', 'County', 'Country', 'Airport_Code', 'Weather_Timestamp', 'Wind_Direction'], axis=1)\n",
    "\n",
    "print(df_refined.describe())\n",
    "print(df_refined.info())\n",
    "print(df_refined.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_all_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Missing Values\n",
    "\n",
    "I've managed missing values in the code blocks below. Here's a brief explanation of my approch to each column:\n",
    "\n",
    "- Temperature, Wind_Chill, Humidity, Pressure, Visibility: imputed based on the mean temp of other accident entries sharing the same day and zip code (or state if there are none in the zip code)\n",
    "- Precipitation, Wind_Speed: assumed NaN indicates no precipitation/wind and replaced NaN with zero\n",
    "- Sunrise_Sunset: imputed based on\n",
    "- Civil_Twilight: imputed based on\n",
    "- Nautical_Twilight: imputed based on\n",
    "- Astronomical_Twilight: imputed based on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refined['Start_Time'] = pd.to_datetime(df_refined['Start_Time'], yearfirst=True, format='mixed')\n",
    "df_refined['End_Time'] = pd.to_datetime(df_refined['End_Time'], yearfirst=True, format='mixed')\n",
    "df_refined['Acc_date'] = df_refined['Start_Time'].dt.date\n",
    "df_refined['Acc_time'] = df_refined['Start_Time'].dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing NaN values in the temperature column\n",
    "\n",
    "def fill_missing_temp(df, date='Acc_date', zip='Zipcode', state='State', temp='Temperature(F)'):\n",
    "    \"\"\"\n",
    "    Replaces NaN humidity values with the mean humidity of entries\n",
    "    with the same date and zip code. If no match exists with date and county,\n",
    "    uses date and state.\n",
    "    \"\"\"\n",
    "    # Step 1: mean humidity for each date-zip combination\n",
    "    df_temp_fill = df_refined.copy()\n",
    "    \n",
    "    temp_means_zip = df_temp_fill.groupby([date, zip])[temp].transform('mean')\n",
    "    \n",
    "    # fill NaN values with the date-zip group mean\n",
    "    df_temp_fill[temp] = df_temp_fill[temp].fillna(temp_means_zip)\n",
    "    \n",
    "    # Step 2: date-state combination for remaining NaNs\n",
    "    if df_temp_fill[temp].isna().any():\n",
    "        remaining = df_temp_fill[temp].isna().sum()\n",
    "        print(f\"Info: {remaining} temperature entries still missing after date-zip fill.\")\n",
    "        print(\"Filling remaining with date-state mean.\")\n",
    "        \n",
    "        temp_means_state = df_temp_fill.groupby([date, state])[temp].transform('mean')\n",
    "        df_temp_fill[temp] = df_temp_fill[temp].fillna(temp_means_state)\n",
    "    \n",
    "    # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "    if df_temp_fill[temp].isna().any():\n",
    "        remaining = df_temp_fill[temp].isna().sum()\n",
    "        print(f\"Warning: {remaining} temperature entries still missing after date-state fill.\")\n",
    "        print(\"Filling remaining with overall mean as last resort.\")\n",
    "        df_temp_fill[temp] = df_temp_fill[temp].fillna(df_temp_fill[temp].mean())\n",
    "    \n",
    "    return df_temp_fill\n",
    "\n",
    "df_temp_fill = fill_missing_temp(df_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying NaNs have been replaced\n",
    "df_temp_fill['Temperature(F)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing NaN-free column back to the original dataframe\n",
    "df_refined['Temperature(F)'] = df_temp_fill['Temperature(F)']\n",
    "print(f\"Entries with NaN Temperature: {df_refined['Temperature(F)'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting working df to free up memory\n",
    "del df_temp_fill\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing NaN values in the wind chill column\n",
    "\n",
    "def fill_missing_windchill(df, date='Acc_date', zip='Zipcode', state='State', windchill='Wind_Chill(F)'):\n",
    "    \"\"\"\n",
    "    Replaces NaN wind chill values with the mean wind chill of entries\n",
    "    with the same date and zip code. If no match exists with date and county,\n",
    "    uses date and state.\n",
    "    \"\"\"\n",
    "    # Step 1: mean temperature for each date-zip combination\n",
    "    df_windchill_fill = df_refined.copy()\n",
    "    \n",
    "    windchill_means_zip = df_windchill_fill.groupby([date, zip])[windchill].transform('mean')\n",
    "    \n",
    "    # fill NaN values with the date-zip group mean\n",
    "    df_windchill_fill[windchill] = df_windchill_fill[windchill].fillna(windchill_means_zip)\n",
    "    \n",
    "    # Step 2: date-state combination for remaining NaNs\n",
    "    if df_windchill_fill[windchill].isna().any():\n",
    "        remaining = df_windchill_fill[windchill].isna().sum()\n",
    "        print(f\"Info: {remaining} wind chills still missing after date-zip fill.\")\n",
    "        print(\"Filling remaining with date-state mean.\")\n",
    "        \n",
    "        windchill_means_state = df_windchill_fill.groupby([date, state])[windchill].transform('mean')\n",
    "        df_windchill_fill[windchill] = df_windchill_fill[windchill].fillna(windchill_means_state)\n",
    "    \n",
    "    # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "    if df_windchill_fill[windchill].isna().any():\n",
    "        remaining = df_windchill_fill[windchill].isna().sum()\n",
    "        print(f\"Warning: {remaining} wind chills still missing after date-state fill.\")\n",
    "        print(\"Filling remaining with overall mean as last resort.\")\n",
    "        df_windchill_fill[windchill] = df_windchill_fill[windchill].fillna(df_windchill_fill[windchill].mean())\n",
    "    \n",
    "    return df_windchill_fill\n",
    "\n",
    "df_windchill_fill = fill_missing_windchill(df_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying NaNs have been replaced in the working df\n",
    "df_windchill_fill['Wind_Chill(F)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing NaN-free column back to the original dataframe\n",
    "df_refined['Wind_Chill(F)'] = df_windchill_fill['Wind_Chill(F)']\n",
    "print(f\"Entries with NaN Wind Chill: {df_refined['Wind_Chill(F)'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting working df to free up memory\n",
    "del df_windchill_fill\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing NaN values in the humidity column\n",
    "\n",
    "def fill_missing_hum(df, date='Acc_date', zip='Zipcode', state='State', hum='Humidity(%)'):\n",
    "    \"\"\"\n",
    "    Replaces NaN humidity values with the mean humidity of entries\n",
    "    with the same date and zip code. If no match exists with date and county,\n",
    "    uses date and state.\n",
    "    \"\"\"\n",
    "    # Step 1: mean humidity for each date-zip combination\n",
    "    df_hum_fill = df_refined.copy()\n",
    "    \n",
    "    hum_means_zip = df_hum_fill.groupby([date, zip])[hum].transform('mean')\n",
    "    \n",
    "    # fill NaN values with the date-zip group mean\n",
    "    df_hum_fill[hum] = df_hum_fill[hum].fillna(hum_means_zip)\n",
    "    \n",
    "    # Step 2: date-state combination for remaining NaNs\n",
    "    if df_hum_fill[hum].isna().any():\n",
    "        remaining = df_hum_fill[hum].isna().sum()\n",
    "        print(f\"Info: {remaining} humidity entries still missing after date-zip fill.\")\n",
    "        print(\"Filling remaining with date-state mean.\")\n",
    "        \n",
    "        hum_means_state = df_hum_fill.groupby([date, state])[hum].transform('mean')\n",
    "        df_hum_fill[hum] = df_hum_fill[hum].fillna(hum_means_state)\n",
    "    \n",
    "    # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "    if df_hum_fill[hum].isna().any():\n",
    "        remaining = df_hum_fill[hum].isna().sum()\n",
    "        print(f\"Warning: {remaining} humidity entries still missing after date-state fill.\")\n",
    "        print(\"Filling remaining with overall mean as last resort.\")\n",
    "        df_hum_fill[hum] = df_hum_fill[hum].fillna(df_hum_fill[hum].mean())\n",
    "    \n",
    "    return df_hum_fill\n",
    "\n",
    "df_hum_fill = fill_missing_hum(df_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying NaNs have been replaced in the working df\n",
    "df_hum_fill['Humidity(%)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing NaN-free column back to the original dataframe\n",
    "df_refined['Humidity(%)'] = df_hum_fill['Humidity(%)']\n",
    "print(f\"Entries with NaN humidity: {df_refined['Humidity(%)'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting working df to free up memory\n",
    "del df_hum_fill\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing NaN values in the pressure column\n",
    "\n",
    "def fill_missing_press(df, date='Acc_date', zip='Zipcode', state='State', press='Pressure(in)'):\n",
    "    \"\"\"\n",
    "    Replaces NaN pressure values with the mean pressure of entries\n",
    "    with the same date and zip code. If no match exists with date and county,\n",
    "    uses date and state.\n",
    "    \"\"\"\n",
    "    # Step 1: mean pressure for each date-zip combination\n",
    "    df_press_fill = df_refined.copy()\n",
    "    \n",
    "    press_means_zip = df_press_fill.groupby([date, zip])[press].transform('mean')\n",
    "    \n",
    "    # fill NaN values with the date-zip group mean\n",
    "    df_press_fill[press] = df_press_fill[press].fillna(press_means_zip)\n",
    "    \n",
    "    # Step 2: date-state combination for remaining NaNs\n",
    "    if df_press_fill[press].isna().any():\n",
    "        remaining = df_press_fill[press].isna().sum()\n",
    "        print(f\"Info: {remaining} pressure entries still missing after date-zip fill.\")\n",
    "        print(\"Filling remaining with date-state mean.\")\n",
    "        \n",
    "        press_means_state = df_press_fill.groupby([date, state])[press].transform('mean')\n",
    "        df_press_fill[press] = df_press_fill[press].fillna(press_means_state)\n",
    "    \n",
    "    # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "    if df_press_fill[press].isna().any():\n",
    "        remaining = df_press_fill[press].isna().sum()\n",
    "        print(f\"Warning: {remaining} pressure entries still missing after date-state fill.\")\n",
    "        print(\"Filling remaining with overall mean as last resort.\")\n",
    "        df_press_fill[press] = df_press_fill[press].fillna(df_press_fill[press].mean())\n",
    "    \n",
    "    return df_press_fill\n",
    "\n",
    "df_press_fill = fill_missing_press(df_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying NaNs have been replaced in the working df\n",
    "df_press_fill['Pressure(in)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing NaN-free column back to the original dataframe\n",
    "df_refined['Pressure(in)'] = df_press_fill['Pressure(in)']\n",
    "print(f\"Entries with NaN pressure: {df_refined['Pressure(in)'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting working df to free up memory\n",
    "del df_press_fill\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing NaN values in the visibility column\n",
    "\n",
    "def fill_missing_vis(df, date='Acc_date', zip='Zipcode', state='State', vis='Visibility(mi)'):\n",
    "    \"\"\"\n",
    "    Replaces NaN visibility values with the mean visibility of entries\n",
    "    with the same date and zip code. If no match exists with date and county,\n",
    "    uses date and state.\n",
    "    \"\"\"\n",
    "    # Step 1: mean visibility for each date-zip combination\n",
    "    df_vis_fill = df_refined.copy()\n",
    "    \n",
    "    vis_means_zip = df_vis_fill.groupby([date, zip])[vis].transform('mean')\n",
    "    \n",
    "    # fill NaN values with the date-zip group mean\n",
    "    df_vis_fill[vis] = df_vis_fill[vis].fillna(vis_means_zip)\n",
    "    \n",
    "    # Step 2: date-state combination for remaining NaNs\n",
    "    if df_vis_fill[vis].isna().any():\n",
    "        remaining = df_vis_fill[vis].isna().sum()\n",
    "        print(f\"Info: {remaining} visibility entries still missing after date-zip fill.\")\n",
    "        print(\"Filling remaining with date-state mean.\")\n",
    "        \n",
    "        vis_means_state = df_vis_fill.groupby([date, state])[vis].transform('mean')\n",
    "        df_vis_fill[vis] = df_vis_fill[vis].fillna(vis_means_state)\n",
    "    \n",
    "    # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "    if df_vis_fill[vis].isna().any():\n",
    "        remaining = df_vis_fill[vis].isna().sum()\n",
    "        print(f\"Warning: {remaining} visibility entries still missing after date-state fill.\")\n",
    "        print(\"Filling remaining with overall mean as last resort.\")\n",
    "        df_vis_fill[vis] = df_vis_fill[vis].fillna(df_vis_fill[vis].mean())\n",
    "    \n",
    "    return df_vis_fill\n",
    "\n",
    "df_vis_fill = fill_missing_vis(df_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying NaNs have been replaced in the working df\n",
    "df_vis_fill['Visibility(mi)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing NaN-free column back to the original dataframe\n",
    "df_refined['Visibility(mi)'] = df_vis_fill['Visibility(mi)']\n",
    "print(f\"Entries with NaN visibility: {df_refined['Visibility(mi)'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting working df to free up memory\n",
    "del df_vis_fill\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing NaN with 0 in wind speed and precipitation\n",
    "\n",
    "df_refined['Wind_Speed(mph)'] = df_refined['Wind_Speed(mph)'].fillna(0)\n",
    "df_refined['Precipitation(in)'] = df_refined['Precipitation(in)'].fillna(0)\n",
    "print(f\"Entries with NaN Wind Speed: {df_refined['Wind_Speed(mph)'].isna().sum()}\")\n",
    "print(f\"Entries with NaN Precipitation: {df_refined['Precipitation(in)'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting boolean to integer\n",
    "df_refined['Amenity'] = df_refined['Amenity'].astype(int)\n",
    "df_refined['Bump'] = df_refined['Bump'].astype(int)\n",
    "df_refined['Crossing'] = df_refined['Crossing'].astype(int)\n",
    "df_refined['Give_Way'] = df_refined['Give_Way'].astype(int)\n",
    "df_refined['Junction'] = df_refined['Junction'].astype(int)\n",
    "df_refined['No_Exit'] = df_refined['No_Exit'].astype(int)\n",
    "df_refined['Railway'] = df_refined['Railway'].astype(int)\n",
    "df_refined['Roundabout'] = df_refined['Roundabout'].astype(int)\n",
    "df_refined['Station'] = df_refined['Station'].astype(int)\n",
    "df_refined['Stop'] = df_refined['Stop'].astype(int)\n",
    "df_refined['Traffic_Calming'] = df_refined['Traffic_Calming'].astype(int)\n",
    "df_refined['Traffic_Signal'] = df_refined['Traffic_Signal'].astype(int)\n",
    "df_refined['Turning_Loop'] = df_refined['Turning_Loop'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking to see if the day/night data points are missing for the same entries\n",
    "day_night_cols =['Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']\n",
    "blank_counts = df_refined[day_night_cols].isna().sum(axis=1)\n",
    "results = blank_counts.value_counts().sort_index()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del blank_counts\n",
    "del results\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating column displaying Day or Night based on a day with equal length days and nights\n",
    "def day_or_night(t):\n",
    "    if time(6, 0, 0) <= t < time(18, 0, 0):\n",
    "        return 'Day'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "df_refined['Day_Night_Calc'] = df_refined['Acc_time'].apply(day_or_night)\n",
    "df_refined['Day_Night_Calc'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refined_dup2 = df_refined.copy()\n",
    "\n",
    "# replacing missing day/night values with the generic day/night column created above\n",
    "for col in day_night_cols:\n",
    "    df_refined_dup2[col] = np.where(df_refined_dup2[col].isna(), df_refined_dup2['Day_Night_Calc'], df_refined_dup2[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying that no empty entries remain\n",
    "blank_counts2 = df_refined_dup2[day_night_cols].isna().sum(axis=1)\n",
    "results2 = blank_counts2.value_counts().sort_index()\n",
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refined['Sunrise_Sunset'] = df_refined_dup2['Sunrise_Sunset']\n",
    "df_refined['Civil_Twilight'] = df_refined_dup2['Civil_Twilight']\n",
    "df_refined['Nautical_Twilight'] = df_refined_dup2['Nautical_Twilight']\n",
    "df_refined['Astronomical_Twilight'] = df_refined_dup2['Astronomical_Twilight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_refined_dup2\n",
    "del results2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Weather Condition\n",
    "\n",
    "The Weather_Condition field has a large number of values across the dataset. Many of these values are used rarely and will complicate modeling, so I'll consolidate them into a smaller number of value options. This will be a manual process-I'll review and create a mapping table by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Count of Unique Weather Condition Entries: {df_all_data['Weather_Condition'].nunique()}\")\n",
    "df_refined['Weather_Condition'].value_counts().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing a list of weather condition unique values to csv\n",
    "unique_wthr_cond = pd.Series(df_refined['Weather_Condition'].unique())\n",
    "unique_wthr_cond.to_csv('conditions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_dict_reader(filename):\n",
    "    result = {}\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if len(row) >=2:\n",
    "                result[row[0]] = row[1]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_mapper = csv_to_dict_reader('conditions_consolidated.csv')\n",
    "df_refined_dup1 = df_refined.copy()\n",
    "\n",
    "df_refined_dup1['Weather_Condition'] = df_refined_dup1['Weather_Condition'].map(weather_mapper)\n",
    "print(df_refined_dup1['Weather_Condition'].value_counts().sort_values(ascending=True))\n",
    "print(df_refined_dup1['Weather_Condition'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refined['Weather_Condition'] = df_refined_dup1['Weather_Condition']\n",
    "df_refined['Weather_Condition'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_refined_dup1\n",
    "del weather_mapper\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing Data\n",
    "\n",
    "I'm adding a feature that categorizes the location of the accidents as urban / suburban / rural. I'm using Rural-Urban Commuting Area Codes from the USDA Economic Research Service as the data source. https://www.ers.usda.gov/data-products/rural-urban-commuting-area-codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_type_mapper = csv_to_dict_reader('category_by_zip.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting zip code entries to the 5-digit format\n",
    "\n",
    "df_refined['Zipcode'] = df_refined['Zipcode'].astype(str)\n",
    "df_refined['Zipcode'] = df_refined['Zipcode'].str[:5]\n",
    "\n",
    "# mapping a new location type field using the mapper created above\n",
    "df_refined['Location_Type'] = df_refined['Zipcode'].map(loc_type_mapper)\n",
    "df_refined['Location_Type'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for clear pattern(s) in the missing location type rows\n",
    "nan_zip_codes = df_refined.loc[df_refined['Location_Type'].isna(), 'Zipcode']\n",
    "nan_zip_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determining proportions of location types in the whole dataset; will impute the NaNs with the same proportion\n",
    "df_refined['Location_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling missing values in line with value distribution in the rest of the dataset\n",
    "location_types = ['Urban', 'Suburban', 'Rural']\n",
    "proportions = [0.83, 0.10, 0.07]\n",
    "\n",
    "num_missing = df_refined['Location_Type'].isna().sum()\n",
    "random_values = np.random.choice(location_types, size=num_missing, p=proportions)\n",
    "\n",
    "df_refined.loc[df_refined['Location_Type'].isna(), 'Location_Type'] = random_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying all rows have been filled\n",
    "num_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del nan_zip_codes\n",
    "del num_missing\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.read_csv('Data/cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing fields created for cleaning and used for imputation and construction \n",
    "df_cleaned = df_clean.drop(['Start_Time', 'End_Time', 'Start_Lat', 'Start_Lng', 'City', 'County', 'State', 'Zipcode', 'Distance(mi)'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = pd.get_dummies(df_cleaned, columns=['Weather_Condition', 'Sunrise_Sunset', 'Civil_Twilight', \n",
    "                                                 'Nautical_Twilight', 'Astronomical_Twilight', 'Location_Type'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_cleaned\n",
    "del df_clean\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting X and y\n",
    "X = df_encoded.drop(['Severity'], axis=1)\n",
    "y = df_encoded['Severity']\n",
    "\n",
    "# splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "   X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# scaling features in case I need something other than decision trees\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_encoded\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Now that the data is preprocessed, we can move to the analysis. As mentioned earlier, I selected a random forest model in order to maximize the interpretability of the results. Interpretability is my priority because of the need to deliver clear busines insights; if the goal was to create an accurate predictive model, I would likely have chosen a different approach.\n",
    "\n",
    "I'll start by creating a baseline model to provide a comparison point for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing a baseline model\n",
    "base_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# cross-validation\n",
    "base_scores = cross_val_score(base_model, X_train_scaled, \n",
    "                              y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# fit baseline model to explore feature importance\n",
    "base_model.fit(X_train_scaled, y_train)\n",
    "initial_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': base_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Baseline model performance\")\n",
    "print(f\"Accuracy: {base_scores.mean():.4f}(+/- {base_scores.std() * 2:.4f})\")\n",
    "print(f\"Training Score: {base_model.score(X_train_scaled, y_train):.4f}\")\n",
    "\n",
    "print(\"\\nInitial Feature Importance:\")\n",
    "print(initial_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model is overfitting: its accuracy on training data is 96% vs. 74% on test (i.e. unseen) data.\n",
    "\n",
    "I'll next look for optimal parameters for this model. I'm using a randomized search to efficiently explore a large parameter space, based on probability distributions around each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "virtual_memory = psutil.virtual_memory()\n",
    "available_gb = round(virtual_memory.available / (1024**3), 2)\n",
    "available_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter distributions for random search\n",
    "param_dist = {\n",
    "   # uniform distribution from 1 to 20 for max_depth\n",
    "   'max_depth': randint(1, 20),\n",
    "  \n",
    "   # log-uniform distribution from 2 to 50 for min_samples_split\n",
    "   'min_samples_split': reciprocal(a=2, b=50).rvs(1000).round().astype(int),\n",
    "  \n",
    "   # log-uniform distribution from 1 to 20 for min_samples_leaf\n",
    "   'min_samples_leaf': reciprocal(a=1, b=20).rvs(1000).round().astype(int),\n",
    "  \n",
    "   # categorical distribution for criterion\n",
    "   'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Initialize randomized search\n",
    "random_search = RandomizedSearchCV(\n",
    "   DecisionTreeClassifier(random_state=42),\n",
    "   param_distributions=param_dist,\n",
    "   n_iter=100,  # Number of parameter settings sampled\n",
    "   cv=5,\n",
    "   scoring='accuracy',\n",
    "   n_jobs=-1,\n",
    "   random_state=42\n",
    ")\n",
    "\n",
    "# Perform random search\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "print(\"Best parameters from random search:\")\n",
    "print(random_search.best_params_)\n",
    "print(f\"Best cross-validation score: {random_search.best_score_:.3f}\")\n",
    "print(f\"Best training score: {random_search.best_estimator_.score(X_train_scaled, y_train):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll now perform a more focused parameter search using grid search. The best values from the random search inform the grid values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust grid based on best parameters from random search\n",
    "focused_param_grid = {\n",
    "  'criterion': ['entropy', 'gini'],\n",
    "   # For max_depth, we'll explore values around 14 (12-16)\n",
    "  # The reasoning is that tree depth often has a sweet spot,\n",
    "  # and we want to make sure 14 is truly optimal\n",
    "  'max_depth': [4, 5, 6, 7, 8],\n",
    "   # Since min_samples_leaf=2 was optimal, we'll explore very small values\n",
    "  # This suggests the model benefits from granular splits\n",
    "  'min_samples_leaf': [5, 7, 9],\n",
    "   # For min_samples_split, we'll create a window around 46\n",
    "  # We'll use smaller steps since we're in a promising region\n",
    "  'min_samples_split': [5, 7, 9]\n",
    "}\n",
    "\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(\n",
    "   DecisionTreeClassifier(random_state=42),\n",
    "   focused_param_grid,\n",
    "   cv=5,\n",
    "   scoring='accuracy',\n",
    "   n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "print(\"Best parameters from grid search:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.3f}\")\n",
    "print(f\"Best training score: {grid_search.best_estimator_.score(X_train_scaled, y_train):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Business Insight/Recommendation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Insight/Recommendation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Insight/Recommendation 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tableau Dashboard link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "Text here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
