{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Add a relevant banner image here](path_to_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "Short project description. Your bottom line up front (BLUF) insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "Text here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load relevant imports here\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import csv\n",
    "from datetime import time\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from scipy.stats import randint, reciprocal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data = pd.read_csv('Data/US_Accidents_March23.csv')\n",
    "print(df_all_data.describe())\n",
    "print(df_all_data.head())\n",
    "print(df_all_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_severity_by_state = pd.crosstab(df_all_data['Severity'], df_all_data['State'])\n",
    "df_severity_by_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart of accident severity by state\n",
    "states = df_severity_by_state.columns\n",
    "sev1 = df_severity_by_state.iloc[0]\n",
    "sev2 = df_severity_by_state.iloc[1]\n",
    "sev3 = df_severity_by_state.iloc[2]\n",
    "sev4 = ct_sev4_by_state = df_severity_by_state.iloc[3]\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.bar(states, sev1)\n",
    "plt.bar(states, sev2, bottom=sev1)\n",
    "plt.bar(states, sev3, bottom=sev1+sev2)\n",
    "plt.bar(states, sev4, bottom=sev1+sev2+sev3)\n",
    "plt.xlabel(\"State\")\n",
    "plt.xticks(rotation=60)\n",
    "plt.ylabel(\"Number of Accidents\")\n",
    "plt.legend([\"Severity 1\", \"Severity 2\", \"Severity 3\", \"Severity 4\"])\n",
    "plt.title(\"Accident Severity by State (2016 - 2023)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chart of accident severity by state without sev2 (sev 2 >> than the others so obscures sev 1, 3, 4 above)\n",
    "states = df_severity_by_state.columns\n",
    "sev1 = df_severity_by_state.iloc[0]\n",
    "sev3 = df_severity_by_state.iloc[2]\n",
    "sev4 = ct_sev4_by_state = df_severity_by_state.iloc[3]\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.bar(states, sev1)\n",
    "plt.bar(states, sev3)\n",
    "plt.bar(states, sev4)\n",
    "plt.xlabel(\"State\")\n",
    "plt.xticks(rotation=60)\n",
    "plt.ylabel(\"Number of Accidents\")\n",
    "plt.legend([\"Severity 1\", \"Severity 3\", \"Severity 4\"])\n",
    "plt.title(\"Accident Severity by State, Excluding Severity 2 (2016 - 2023)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sev_by_crossing = pd.crosstab(df_all_data['Severity'], df_all_data['Crossing'])\n",
    "df_sev_by_crossing = df_sev_by_crossing.rename(columns={False: \"No\", True: \"Yes\"})\n",
    "df_sev_by_crossing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# charts of accident severity by bump, traffic calming, roundabout\n",
    "# chart of accident severity by state\n",
    "crossing = df_sev_by_crossing.columns\n",
    "cr1 = df_sev_by_crossing.iloc[0]\n",
    "cr2 = df_sev_by_crossing.iloc[1]\n",
    "cr3 = df_sev_by_crossing.iloc[2]\n",
    "cr4 = df_sev_by_crossing.iloc[3]\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.bar(crossing, cr1)\n",
    "plt.bar(crossing, cr2)\n",
    "plt.bar(crossing, cr3)\n",
    "plt.bar(crossing, cr4)\n",
    "plt.xticks(rotation=60)\n",
    "plt.xlabel(\"Nearby Crossing\")\n",
    "plt.ylabel(\"Number of Accidents\")\n",
    "plt.legend([\"Severity 1\", \"Severity 2\", \"Severity 3\", \"Severity 4\"])\n",
    "plt.title(\"Accident Severity by Proximity to Crossing, (2016 - 2023)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Count of Wind Direction Entries: {df_all_data['Wind_Direction'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Selection\n",
    "\n",
    "Based on my exploration of the data, I'm dropping the following fields from the dataset for the following reasons:\n",
    "\n",
    "- Source: contains information that has no relationship to causes and effects of accidents\n",
    "- Start_Lat, Start_Lng, End_Lat, End_Lng, Street, City, County, State, Zipcode, Country: I'm going to model factors that influence accident severity independent of location.\n",
    "    - I'll drop Zipcode and State data cleaning and construction since they'll be used for imputation and creating a new field\n",
    "- Start_Time, End_Time: as with location features, I'm dropping these because I'll model factors independent of time\n",
    "    - I'll drop Start_Time after imputation\n",
    "- Description: unstructured text that will not give meaningful results with the planned modeling\n",
    "- Timezone: duplicates zip/state with less precision\n",
    "- Street: this field is not standardized and will introduce noise to the modeling\n",
    "- Country: all data is from the United States so this field is redundant\n",
    "- Airport_Code: doesn't provide germane information-the exact location where weather conditions are reported is not a variable that can be adjusted\n",
    "- Weather_Timestamp: not related to the conditions of the accidents in any way\n",
    "- Wind_Direction: too many unique values; values are also not related to travel directions so it's unlikely they'll  produce clear/actionable conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refined = df_all_data.drop(['ID', 'Source', 'Start_Lat', 'Start_Lng', 'End_Lat', 'End_Lng', 'Description',\n",
    "                                'Timezone', 'Street', 'City', 'County', 'Country', 'Airport_Code', 'Weather_Timestamp', 'Wind_Direction'], axis=1)\n",
    "\n",
    "print(df_refined.describe())\n",
    "print(df_refined.info())\n",
    "print(df_refined.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_all_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Missing Values\n",
    "\n",
    "I've managed missing values in the code blocks below. Here's a brief explanation of my approch to each column:\n",
    "\n",
    "- Temperature, Wind_Chill, Humidity, Pressure, Visibility: imputed based on the mean temp of other accident entries sharing the same day and zip code (or state if there are none in the zip code)\n",
    "- Precipitation, Wind_Speed: assumed NaN indicates no precipitation/wind and replaced NaN with zero\n",
    "- Sunrise_Sunset: imputed based on\n",
    "- Civil_Twilight: imputed based on\n",
    "- Nautical_Twilight: imputed based on\n",
    "- Astronomical_Twilight: imputed based on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refined['Start_Time'] = pd.to_datetime(df_refined['Start_Time'], yearfirst=True, format='mixed')\n",
    "df_refined['End_Time'] = pd.to_datetime(df_refined['End_Time'], yearfirst=True, format='mixed')\n",
    "df_refined['Acc_date'] = df_refined['Start_Time'].dt.date\n",
    "df_refined['Acc_time'] = df_refined['Start_Time'].dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing NaN values in the temperature column\n",
    "\n",
    "def fill_missing_temp(df, date='Acc_date', zip='Zipcode', state='State', temp='Temperature(F)'):\n",
    "    \"\"\"\n",
    "    Replaces NaN humidity values with the mean humidity of entries\n",
    "    with the same date and zip code. If no match exists with date and county,\n",
    "    uses date and state.\n",
    "    \"\"\"\n",
    "    # Step 1: mean humidity for each date-zip combination\n",
    "    df_temp_fill = df_refined.copy()\n",
    "    \n",
    "    temp_means_zip = df_temp_fill.groupby([date, zip])[temp].transform('mean')\n",
    "    \n",
    "    # fill NaN values with the date-zip group mean\n",
    "    df_temp_fill[temp] = df_temp_fill[temp].fillna(temp_means_zip)\n",
    "    \n",
    "    # Step 2: date-state combination for remaining NaNs\n",
    "    if df_temp_fill[temp].isna().any():\n",
    "        remaining = df_temp_fill[temp].isna().sum()\n",
    "        print(f\"Info: {remaining} temperature entries still missing after date-zip fill.\")\n",
    "        print(\"Filling remaining with date-state mean.\")\n",
    "        \n",
    "        temp_means_state = df_temp_fill.groupby([date, state])[temp].transform('mean')\n",
    "        df_temp_fill[temp] = df_temp_fill[temp].fillna(temp_means_state)\n",
    "    \n",
    "    # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "    if df_temp_fill[temp].isna().any():\n",
    "        remaining = df_temp_fill[temp].isna().sum()\n",
    "        print(f\"Warning: {remaining} temperature entries still missing after date-state fill.\")\n",
    "        print(\"Filling remaining with overall mean as last resort.\")\n",
    "        df_temp_fill[temp] = df_temp_fill[temp].fillna(df_temp_fill[temp].mean())\n",
    "    \n",
    "    return df_temp_fill\n",
    "\n",
    "df_temp_fill = fill_missing_temp(df_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying NaNs have been replaced\n",
    "df_temp_fill['Temperature(F)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing NaN-free column back to the original dataframe\n",
    "df_refined['Temperature(F)'] = df_temp_fill['Temperature(F)']\n",
    "print(f\"Entries with NaN Temperature: {df_refined['Temperature(F)'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting working df to free up memory\n",
    "del df_temp_fill\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing NaN values in the wind chill column\n",
    "\n",
    "def fill_missing_windchill(df, date='Acc_date', zip='Zipcode', state='State', windchill='Wind_Chill(F)'):\n",
    "    \"\"\"\n",
    "    Replaces NaN wind chill values with the mean wind chill of entries\n",
    "    with the same date and zip code. If no match exists with date and county,\n",
    "    uses date and state.\n",
    "    \"\"\"\n",
    "    # Step 1: mean temperature for each date-zip combination\n",
    "    df_windchill_fill = df_refined.copy()\n",
    "    \n",
    "    windchill_means_zip = df_windchill_fill.groupby([date, zip])[windchill].transform('mean')\n",
    "    \n",
    "    # fill NaN values with the date-zip group mean\n",
    "    df_windchill_fill[windchill] = df_windchill_fill[windchill].fillna(windchill_means_zip)\n",
    "    \n",
    "    # Step 2: date-state combination for remaining NaNs\n",
    "    if df_windchill_fill[windchill].isna().any():\n",
    "        remaining = df_windchill_fill[windchill].isna().sum()\n",
    "        print(f\"Info: {remaining} wind chills still missing after date-zip fill.\")\n",
    "        print(\"Filling remaining with date-state mean.\")\n",
    "        \n",
    "        windchill_means_state = df_windchill_fill.groupby([date, state])[windchill].transform('mean')\n",
    "        df_windchill_fill[windchill] = df_windchill_fill[windchill].fillna(windchill_means_state)\n",
    "    \n",
    "    # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "    if df_windchill_fill[windchill].isna().any():\n",
    "        remaining = df_windchill_fill[windchill].isna().sum()\n",
    "        print(f\"Warning: {remaining} wind chills still missing after date-state fill.\")\n",
    "        print(\"Filling remaining with overall mean as last resort.\")\n",
    "        df_windchill_fill[windchill] = df_windchill_fill[windchill].fillna(df_windchill_fill[windchill].mean())\n",
    "    \n",
    "    return df_windchill_fill\n",
    "\n",
    "df_windchill_fill = fill_missing_windchill(df_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying NaNs have been replaced in the working df\n",
    "df_windchill_fill['Wind_Chill(F)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing NaN-free column back to the original dataframe\n",
    "df_refined['Wind_Chill(F)'] = df_windchill_fill['Wind_Chill(F)']\n",
    "print(f\"Entries with NaN Wind Chill: {df_refined['Wind_Chill(F)'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting working df to free up memory\n",
    "del df_windchill_fill\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing NaN values in the humidity column\n",
    "\n",
    "def fill_missing_hum(df, date='Acc_date', zip='Zipcode', state='State', hum='Humidity(%)'):\n",
    "    \"\"\"\n",
    "    Replaces NaN humidity values with the mean humidity of entries\n",
    "    with the same date and zip code. If no match exists with date and county,\n",
    "    uses date and state.\n",
    "    \"\"\"\n",
    "    # Step 1: mean humidity for each date-zip combination\n",
    "    df_hum_fill = df_refined.copy()\n",
    "    \n",
    "    hum_means_zip = df_hum_fill.groupby([date, zip])[hum].transform('mean')\n",
    "    \n",
    "    # fill NaN values with the date-zip group mean\n",
    "    df_hum_fill[hum] = df_hum_fill[hum].fillna(hum_means_zip)\n",
    "    \n",
    "    # Step 2: date-state combination for remaining NaNs\n",
    "    if df_hum_fill[hum].isna().any():\n",
    "        remaining = df_hum_fill[hum].isna().sum()\n",
    "        print(f\"Info: {remaining} humidity entries still missing after date-zip fill.\")\n",
    "        print(\"Filling remaining with date-state mean.\")\n",
    "        \n",
    "        hum_means_state = df_hum_fill.groupby([date, state])[hum].transform('mean')\n",
    "        df_hum_fill[hum] = df_hum_fill[hum].fillna(hum_means_state)\n",
    "    \n",
    "    # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "    if df_hum_fill[hum].isna().any():\n",
    "        remaining = df_hum_fill[hum].isna().sum()\n",
    "        print(f\"Warning: {remaining} humidity entries still missing after date-state fill.\")\n",
    "        print(\"Filling remaining with overall mean as last resort.\")\n",
    "        df_hum_fill[hum] = df_hum_fill[hum].fillna(df_hum_fill[hum].mean())\n",
    "    \n",
    "    return df_hum_fill\n",
    "\n",
    "df_hum_fill = fill_missing_hum(df_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying NaNs have been replaced in the working df\n",
    "df_hum_fill['Humidity(%)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing NaN-free column back to the original dataframe\n",
    "df_refined['Humidity(%)'] = df_hum_fill['Humidity(%)']\n",
    "print(f\"Entries with NaN humidity: {df_refined['Humidity(%)'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting working df to free up memory\n",
    "del df_hum_fill\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing NaN values in the pressure column\n",
    "\n",
    "def fill_missing_press(df, date='Acc_date', zip='Zipcode', state='State', press='Pressure(in)'):\n",
    "    \"\"\"\n",
    "    Replaces NaN pressure values with the mean pressure of entries\n",
    "    with the same date and zip code. If no match exists with date and county,\n",
    "    uses date and state.\n",
    "    \"\"\"\n",
    "    # Step 1: mean pressure for each date-zip combination\n",
    "    df_press_fill = df_refined.copy()\n",
    "    \n",
    "    press_means_zip = df_press_fill.groupby([date, zip])[press].transform('mean')\n",
    "    \n",
    "    # fill NaN values with the date-zip group mean\n",
    "    df_press_fill[press] = df_press_fill[press].fillna(press_means_zip)\n",
    "    \n",
    "    # Step 2: date-state combination for remaining NaNs\n",
    "    if df_press_fill[press].isna().any():\n",
    "        remaining = df_press_fill[press].isna().sum()\n",
    "        print(f\"Info: {remaining} pressure entries still missing after date-zip fill.\")\n",
    "        print(\"Filling remaining with date-state mean.\")\n",
    "        \n",
    "        press_means_state = df_press_fill.groupby([date, state])[press].transform('mean')\n",
    "        df_press_fill[press] = df_press_fill[press].fillna(press_means_state)\n",
    "    \n",
    "    # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "    if df_press_fill[press].isna().any():\n",
    "        remaining = df_press_fill[press].isna().sum()\n",
    "        print(f\"Warning: {remaining} pressure entries still missing after date-state fill.\")\n",
    "        print(\"Filling remaining with overall mean as last resort.\")\n",
    "        df_press_fill[press] = df_press_fill[press].fillna(df_press_fill[press].mean())\n",
    "    \n",
    "    return df_press_fill\n",
    "\n",
    "df_press_fill = fill_missing_press(df_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying NaNs have been replaced in the working df\n",
    "df_press_fill['Pressure(in)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing NaN-free column back to the original dataframe\n",
    "df_refined['Pressure(in)'] = df_press_fill['Pressure(in)']\n",
    "print(f\"Entries with NaN pressure: {df_refined['Pressure(in)'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting working df to free up memory\n",
    "del df_press_fill\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing NaN values in the visibility column\n",
    "\n",
    "def fill_missing_vis(df, date='Acc_date', zip='Zipcode', state='State', vis='Visibility(mi)'):\n",
    "    \"\"\"\n",
    "    Replaces NaN visibility values with the mean visibility of entries\n",
    "    with the same date and zip code. If no match exists with date and county,\n",
    "    uses date and state.\n",
    "    \"\"\"\n",
    "    # Step 1: mean visibility for each date-zip combination\n",
    "    df_vis_fill = df_refined.copy()\n",
    "    \n",
    "    vis_means_zip = df_vis_fill.groupby([date, zip])[vis].transform('mean')\n",
    "    \n",
    "    # fill NaN values with the date-zip group mean\n",
    "    df_vis_fill[vis] = df_vis_fill[vis].fillna(vis_means_zip)\n",
    "    \n",
    "    # Step 2: date-state combination for remaining NaNs\n",
    "    if df_vis_fill[vis].isna().any():\n",
    "        remaining = df_vis_fill[vis].isna().sum()\n",
    "        print(f\"Info: {remaining} visibility entries still missing after date-zip fill.\")\n",
    "        print(\"Filling remaining with date-state mean.\")\n",
    "        \n",
    "        vis_means_state = df_vis_fill.groupby([date, state])[vis].transform('mean')\n",
    "        df_vis_fill[vis] = df_vis_fill[vis].fillna(vis_means_state)\n",
    "    \n",
    "    # Step 3: if any NaNs still remain, fill with overall mean as last resort\n",
    "    if df_vis_fill[vis].isna().any():\n",
    "        remaining = df_vis_fill[vis].isna().sum()\n",
    "        print(f\"Warning: {remaining} visibility entries still missing after date-state fill.\")\n",
    "        print(\"Filling remaining with overall mean as last resort.\")\n",
    "        df_vis_fill[vis] = df_vis_fill[vis].fillna(df_vis_fill[vis].mean())\n",
    "    \n",
    "    return df_vis_fill\n",
    "\n",
    "df_vis_fill = fill_missing_vis(df_refined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying NaNs have been replaced in the working df\n",
    "df_vis_fill['Visibility(mi)'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing NaN-free column back to the original dataframe\n",
    "df_refined['Visibility(mi)'] = df_vis_fill['Visibility(mi)']\n",
    "print(f\"Entries with NaN visibility: {df_refined['Visibility(mi)'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting working df to free up memory\n",
    "del df_vis_fill\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing NaN with 0 in wind speed and precipitation\n",
    "\n",
    "df_refined['Wind_Speed(mph)'] = df_refined['Wind_Speed(mph)'].fillna(0)\n",
    "df_refined['Precipitation(in)'] = df_refined['Precipitation(in)'].fillna(0)\n",
    "print(f\"Entries with NaN Wind Speed: {df_refined['Wind_Speed(mph)'].isna().sum()}\")\n",
    "print(f\"Entries with NaN Precipitation: {df_refined['Precipitation(in)'].isna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting boolean to integer\n",
    "df_refined['Amenity'] = df_refined['Amenity'].astype(int)\n",
    "df_refined['Bump'] = df_refined['Bump'].astype(int)\n",
    "df_refined['Crossing'] = df_refined['Crossing'].astype(int)\n",
    "df_refined['Give_Way'] = df_refined['Give_Way'].astype(int)\n",
    "df_refined['Junction'] = df_refined['Junction'].astype(int)\n",
    "df_refined['No_Exit'] = df_refined['No_Exit'].astype(int)\n",
    "df_refined['Railway'] = df_refined['Railway'].astype(int)\n",
    "df_refined['Roundabout'] = df_refined['Roundabout'].astype(int)\n",
    "df_refined['Station'] = df_refined['Station'].astype(int)\n",
    "df_refined['Stop'] = df_refined['Stop'].astype(int)\n",
    "df_refined['Traffic_Calming'] = df_refined['Traffic_Calming'].astype(int)\n",
    "df_refined['Traffic_Signal'] = df_refined['Traffic_Signal'].astype(int)\n",
    "df_refined['Turning_Loop'] = df_refined['Turning_Loop'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking to see if the day/night data points are missing for the same entries\n",
    "day_night_cols =['Sunrise_Sunset', 'Civil_Twilight', 'Nautical_Twilight', 'Astronomical_Twilight']\n",
    "blank_counts = df_refined[day_night_cols].isna().sum(axis=1)\n",
    "results = blank_counts.value_counts().sort_index()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del blank_counts\n",
    "del results\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating column displaying Day or Night based on a day with equal length days and nights\n",
    "def day_or_night(t):\n",
    "    if time(6, 0, 0) <= t < time(18, 0, 0):\n",
    "        return 'Day'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "df_refined['Day_Night_Calc'] = df_refined['Acc_time'].apply(day_or_night)\n",
    "df_refined['Day_Night_Calc'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refined_dup2 = df_refined.copy()\n",
    "\n",
    "# replacing missing day/night values with the generic day/night column created above\n",
    "for col in day_night_cols:\n",
    "    df_refined_dup2[col] = np.where(df_refined_dup2[col].isna(), df_refined_dup2['Day_Night_Calc'], df_refined_dup2[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying that no empty entries remain\n",
    "blank_counts2 = df_refined_dup2[day_night_cols].isna().sum(axis=1)\n",
    "results2 = blank_counts2.value_counts().sort_index()\n",
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refined['Sunrise_Sunset'] = df_refined_dup2['Sunrise_Sunset']\n",
    "df_refined['Civil_Twilight'] = df_refined_dup2['Civil_Twilight']\n",
    "df_refined['Nautical_Twilight'] = df_refined_dup2['Nautical_Twilight']\n",
    "df_refined['Astronomical_Twilight'] = df_refined_dup2['Astronomical_Twilight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_refined_dup2\n",
    "del results2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Weather Condition\n",
    "\n",
    "The Weather_Condition field has a large number of values across the dataset. Many of these values are used rarely and will complicate modeling, so I'll consolidate them into a smaller number of value options. This will be a manual process-I'll review and create a mapping table by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Count of Unique Weather Condition Entries: {df_all_data['Weather_Condition'].nunique()}\")\n",
    "df_refined['Weather_Condition'].value_counts().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing a list of weather condition unique values to csv\n",
    "unique_wthr_cond = pd.Series(df_refined['Weather_Condition'].unique())\n",
    "unique_wthr_cond.to_csv('conditions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_dict_reader(filename):\n",
    "    result = {}\n",
    "    with open(filename, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if len(row) >=2:\n",
    "                result[row[0]] = row[1]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_mapper = csv_to_dict_reader('conditions_consolidated.csv')\n",
    "df_refined_dup1 = df_refined.copy()\n",
    "\n",
    "df_refined_dup1['Weather_Condition'] = df_refined_dup1['Weather_Condition'].map(weather_mapper)\n",
    "print(df_refined_dup1['Weather_Condition'].value_counts().sort_values(ascending=True))\n",
    "print(df_refined_dup1['Weather_Condition'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refined['Weather_Condition'] = df_refined_dup1['Weather_Condition']\n",
    "df_refined['Weather_Condition'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_refined_dup1\n",
    "del weather_mapper\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing Data\n",
    "\n",
    "I'm adding a feature that categorizes the location of the accidents as urban / suburban / rural. I'm using Rural-Urban Commuting Area Codes from the USDA Economic Research Service as the data source. https://www.ers.usda.gov/data-products/rural-urban-commuting-area-codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_type_mapper = csv_to_dict_reader('category_by_zip.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting zip code entries to the 5-digit format\n",
    "\n",
    "df_refined['Zipcode'] = df_refined['Zipcode'].astype(str)\n",
    "df_refined['Zipcode'] = df_refined['Zipcode'].str[:5]\n",
    "\n",
    "# mapping a new location type field using the mapper created above\n",
    "df_refined['Location_Type'] = df_refined['Zipcode'].map(loc_type_mapper)\n",
    "df_refined['Location_Type'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for clear pattern(s) in the missing location type rows\n",
    "nan_zip_codes = df_refined.loc[df_refined['Location_Type'].isna(), 'Zipcode']\n",
    "nan_zip_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determining proportions of location types in the whole dataset; will impute the NaNs with the same proportion\n",
    "df_refined['Location_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling missing values in line with value distribution in the rest of the dataset\n",
    "location_types = ['Urban', 'Suburban', 'Rural']\n",
    "proportions = [0.83, 0.10, 0.07]\n",
    "\n",
    "num_missing = df_refined['Location_Type'].isna().sum()\n",
    "random_values = np.random.choice(location_types, size=num_missing, p=proportions)\n",
    "\n",
    "df_refined.loc[df_refined['Location_Type'].isna(), 'Location_Type'] = random_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifying all rows have been filled\n",
    "num_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del nan_zip_codes\n",
    "del num_missing\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_refined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.read_csv('Data/cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing fields created for cleaning and used for imputation and construction \n",
    "df_cleaned = df_clean.drop(['Start_Time', 'End_Time', 'Start_Lat', 'Start_Lng', 'City', 'County', 'State', 'Zipcode', 'Distance(mi)', 'Wind_Chill(F)'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = pd.get_dummies(df_cleaned, columns=['Weather_Condition', 'Sunrise_Sunset', 'Civil_Twilight', \n",
    "                                                 'Nautical_Twilight', 'Astronomical_Twilight', 'Location_Type'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2413"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_cleaned\n",
    "del df_clean\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting X and y\n",
    "X = df_encoded.drop(['Severity'], axis=1)\n",
    "y = df_encoded['Severity']\n",
    "\n",
    "# splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "   X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# scaling features in case I need something other than decision trees\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_encoded\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Now that the data is preprocessed, we can move to the analysis. As mentioned earlier, I selected a random forest model in order to maximize the interpretability of the results. Interpretability is my priority because of the need to deliver clear busines insights; if the goal was to create an accurate predictive model, I would likely have chosen a different approach.\n",
    "\n",
    "I'll start by creating a baseline model to provide a comparison point for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model performance\n",
      "Accuracy: 0.7395(+/- 0.0004)\n",
      "Training Score: 0.9587\n",
      "\n",
      "Initial Feature Importance:\n",
      "                                 feature  importance\n",
      "2                           Pressure(in)    0.246712\n",
      "0                         Temperature(F)    0.216213\n",
      "1                            Humidity(%)    0.214011\n",
      "4                        Wind_Speed(mph)    0.107764\n",
      "3                         Visibility(mi)    0.045336\n",
      "10                              Junction    0.021777\n",
      "5                      Precipitation(in)    0.016974\n",
      "44                  Sunrise_Sunset_Night    0.015370\n",
      "20              Weather_Condition_Cloudy    0.012282\n",
      "8                               Crossing    0.011274\n",
      "45                  Civil_Twilight_Night    0.010592\n",
      "36       Weather_Condition_Partly Cloudy    0.010226\n",
      "46               Nautical_Twilight_Night    0.008669\n",
      "17                        Traffic_Signal    0.007259\n",
      "47           Astronomical_Twilight_Night    0.006267\n",
      "49                   Location_Type_Urban    0.006217\n",
      "21                Weather_Condition_Fair    0.005729\n",
      "31          Weather_Condition_Light Rain    0.005505\n",
      "48                Location_Type_Suburban    0.004488\n",
      "15                                  Stop    0.004144\n",
      "14                               Station    0.003670\n",
      "12                               Railway    0.002650\n",
      "6                                Amenity    0.002279\n",
      "37                Weather_Condition_Rain    0.001865\n",
      "9                               Give_Way    0.001717\n",
      "25                Weather_Condition_Hazy    0.001251\n",
      "22                 Weather_Condition_Fog    0.001135\n",
      "33          Weather_Condition_Light Snow    0.001109\n",
      "41        Weather_Condition_Thunderstorm    0.001077\n",
      "11                               No_Exit    0.001032\n",
      "34  Weather_Condition_Light Thunderstorm    0.001008\n",
      "26          Weather_Condition_Heavy Rain    0.000850\n",
      "42             Weather_Condition_Tornado    0.000669\n",
      "39               Weather_Condition_Smoke    0.000482\n",
      "28  Weather_Condition_Heavy Thunderstorm    0.000378\n",
      "16                       Traffic_Calming    0.000339\n",
      "29           Weather_Condition_Light Fog    0.000302\n",
      "40                Weather_Condition_Snow    0.000292\n",
      "43         Weather_Condition_Wintry  Mix    0.000283\n",
      "32         Weather_Condition_Light Sleet    0.000247\n",
      "35                Weather_Condition_Mist    0.000191\n",
      "7                                   Bump    0.000125\n",
      "27          Weather_Condition_Heavy Snow    0.000101\n",
      "30  Weather_Condition_Light Freezing Fog    0.000046\n",
      "19        Weather_Condition_Blowing Snow    0.000035\n",
      "23       Weather_Condition_Freezing Rain    0.000024\n",
      "38               Weather_Condition_Sleet    0.000016\n",
      "13                            Roundabout    0.000013\n",
      "24                Weather_Condition_Hail    0.000007\n",
      "18                          Turning_Loop    0.000000\n"
     ]
    }
   ],
   "source": [
    "# initializing a baseline model\n",
    "base_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# cross-validation\n",
    "base_scores = cross_val_score(base_model, X_train_scaled, \n",
    "                              y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# fit baseline model to explore feature importance\n",
    "base_model.fit(X_train_scaled, y_train)\n",
    "initial_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': base_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Baseline model performance\")\n",
    "print(f\"Accuracy: {base_scores.mean():.4f}(+/- {base_scores.std() * 2:.4f})\")\n",
    "print(f\"Training Score: {base_model.score(X_train_scaled, y_train):.4f}\")\n",
    "\n",
    "print(\"\\nInitial Feature Importance:\")\n",
    "print(initial_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model is overfitting: its accuracy on training data is 96% vs. 74% on test (i.e. unseen) data.\n",
    "\n",
    "I'll next look for optimal parameters for this model. I'm using a randomized search to efficiently explore a large parameter space, based on probability distributions around each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.51"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "virtual_memory = psutil.virtual_memory()\n",
    "available_gb = round(virtual_memory.available / (1024**3), 2)\n",
    "available_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.84 GiB for an array with shape (4946172, 50) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 490, in _process_worker\n    r = call_item()\n  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\joblib\\parallel.py\", line 607, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\joblib\\parallel.py\", line 607, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 139, in __call__\n    return self.function(*args, **kwargs)\n  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 858, in _fit_and_score\n    X_train, y_train = _safe_split(estimator, X, y, train)\n  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 156, in _safe_split\n    X_subset = _safe_indexing(X, indices)\n  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\utils\\_indexing.py\", line 270, in _safe_indexing\n    return _array_indexing(X, indices, indices_dtype, axis=axis)\n  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\utils\\_indexing.py\", line 36, in _array_indexing\n    return array[key, ...] if axis == 0 else array[:, key]\n  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\numpy\\core\\memmap.py\", line 334, in __getitem__\n    res = super().__getitem__(index)\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.84 GiB for an array with shape (4946172, 50) and data type float64\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 28\u001b[0m\n\u001b[0;32m     17\u001b[0m random_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[0;32m     18\u001b[0m    DecisionTreeClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),\n\u001b[0;32m     19\u001b[0m    param_distributions\u001b[38;5;241m=\u001b[39mparam_dist,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m    random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Perform random search\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43mrandom_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters from random search:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(random_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1951\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1951\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1952\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1953\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m   1954\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1955\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    965\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    966\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    967\u001b[0m         )\n\u001b[0;32m    968\u001b[0m     )\n\u001b[1;32m--> 970\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\joblib\\parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\joblib\\parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\joblib\\parallel.py:1784\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1778\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1779\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1784\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m     nb_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\joblib\\parallel.py:1859\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[0;32m   1856\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1858\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1859\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\joblib\\parallel.py:758\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    752\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    756\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\joblib\\parallel.py:773\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 773\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.84 GiB for an array with shape (4946172, 50) and data type float64"
     ]
    }
   ],
   "source": [
    "# Define parameter distributions for random search\n",
    "param_dist = {\n",
    "   # uniform distribution from 1 to 20 for max_depth\n",
    "   'max_depth': randint(1, 20),\n",
    "  \n",
    "   # log-uniform distribution from 2 to 50 for min_samples_split\n",
    "   'min_samples_split': reciprocal(a=2, b=50).rvs(1000).round().astype(int),\n",
    "  \n",
    "   # log-uniform distribution from 1 to 20 for min_samples_leaf\n",
    "   'min_samples_leaf': reciprocal(a=1, b=20).rvs(1000).round().astype(int),\n",
    "  \n",
    "   # categorical distribution for criterion\n",
    "   'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Initialize randomized search\n",
    "random_search = RandomizedSearchCV(\n",
    "   DecisionTreeClassifier(random_state=42),\n",
    "   param_distributions=param_dist,\n",
    "   n_iter=100,  # Number of parameter settings sampled\n",
    "   cv=5,\n",
    "   scoring='accuracy',\n",
    "   n_jobs=-1,\n",
    "   random_state=42\n",
    ")\n",
    "\n",
    "# Perform random search\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "print(\"Best parameters from random search:\")\n",
    "print(random_search.best_params_)\n",
    "print(f\"Best cross-validation score: {random_search.best_score_:.3f}\")\n",
    "print(f\"Best training score: {random_search.best_estimator_.score(X_train_scaled, y_train):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll now perform a more focused parameter search using grid search. The best values from the random search inform the grid values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.92 GiB for an array with shape (4946172, 52) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 490, in _process_worker\n    r = call_item()\n  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\joblib\\parallel.py\", line 607, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\joblib\\parallel.py\", line 607, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 139, in __call__\n    return self.function(*args, **kwargs)\n  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 858, in _fit_and_score\n    X_train, y_train = _safe_split(estimator, X, y, train)\n  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 156, in _safe_split\n    X_subset = _safe_indexing(X, indices)\n  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\utils\\_indexing.py\", line 270, in _safe_indexing\n    return _array_indexing(X, indices, indices_dtype, axis=axis)\n  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\utils\\_indexing.py\", line 36, in _array_indexing\n    return array[key, ...] if axis == 0 else array[:, key]\n  File \"c:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\numpy\\core\\memmap.py\", line 334, in __getitem__\n    res = super().__getitem__(index)\nnumpy.core._exceptions._ArrayMemoryError: Unable to allocate 1.92 GiB for an array with shape (4946172, 52) and data type float64\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 18\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Perform grid search\u001b[39;00m\n\u001b[0;32m     10\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m     11\u001b[0m    DecisionTreeClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),\n\u001b[0;32m     12\u001b[0m    focused_param_grid,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m    n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     16\u001b[0m )\n\u001b[1;32m---> 18\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters from grid search:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1571\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    965\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    966\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    967\u001b[0m         )\n\u001b[0;32m    968\u001b[0m     )\n\u001b[1;32m--> 970\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\joblib\\parallel.py:2072\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2066\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2067\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2068\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2070\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\joblib\\parallel.py:1682\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1679\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1681\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1682\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1685\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1687\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\joblib\\parallel.py:1784\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1778\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[0;32m   1779\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[1;32m-> 1784\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m     nb_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\joblib\\parallel.py:1859\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[0;32m   1856\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[0;32m   1858\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1859\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\joblib\\parallel.py:758\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    752\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[0;32m    754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[0;32m    756\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[1;32m--> 758\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\marha\\.conda\\envs\\ai-environment\\lib\\site-packages\\joblib\\parallel.py:773\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[1;32m--> 773\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    774\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[0;32m    775\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.92 GiB for an array with shape (4946172, 52) and data type float64"
     ]
    }
   ],
   "source": [
    "# Adjust grid based on best parameters from random search\n",
    "focused_param_grid = {\n",
    "  'criterion': ['entropy', 'gini'],\n",
    "   # For max_depth, we'll explore values around 14 (12-16)\n",
    "  # The reasoning is that tree depth often has a sweet spot,\n",
    "  # and we want to make sure 14 is truly optimal\n",
    "  'max_depth': [4, 5, 6, 7, 8],\n",
    "   # Since min_samples_leaf=2 was optimal, we'll explore very small values\n",
    "  # This suggests the model benefits from granular splits\n",
    "  'min_samples_leaf': [5, 7, 9],\n",
    "   # For min_samples_split, we'll create a window around 46\n",
    "  # We'll use smaller steps since we're in a promising region\n",
    "  'min_samples_split': [5, 7, 9]\n",
    "}\n",
    "\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(\n",
    "   DecisionTreeClassifier(random_state=42),\n",
    "   focused_param_grid,\n",
    "   cv=5,\n",
    "   scoring='accuracy',\n",
    "   n_jobs=-1\n",
    ")\n",
    "\n",
    "\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "print(\"Best parameters from grid search:\")\n",
    "print(grid_search.best_params_)\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.3f}\")\n",
    "print(f\"Best training score: {grid_search.best_estimator_.score(X_train_scaled, y_train):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "### Business Insight/Recommendation 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Insight/Recommendation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Insight/Recommendation 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tableau Dashboard link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "Text here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
